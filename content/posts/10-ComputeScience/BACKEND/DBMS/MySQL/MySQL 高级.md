---
title: MySQL 高级
author: cupid5trick
created: 2022-11-03 22:29
tags: 
categories: 
access: private
draft: true
lang:
- zh-cn
- en-us
abstract:
keywords:
---




[MySQL: Welcome](https://dev.mysql.com/doc/dev/mysql-server/latest/): <https://dev.mysql.com/doc/dev/mysql-server/latest/>


# 1. 索引的数据结构

## 1.1 为什么要使用索引

索引是存储引擎用于快速找到数据记录的数据结构。在 MySQL 中，首先查看查询条件是否命中某条索引，符合则**通过索引查找**相关数据，不符合则**全表扫描**。

![image-20220301094159045](image-20220301094159045.png)

![image-20220301094325627](image-20220301094325627.png)

添加索引的过程就相当于在硬盘上维护了一个索引的数据结构，即这个**二叉搜索树**。

**索引的目的就是为了减少磁盘的 I/O 次数，加快查询效率。**

## 1.2 索引及其优缺点

### 1.2.1 索引概述

索引是帮助 MySQL 高效获取数据的数据结构。

索引的本质是**排好序的快速查找数据结构**，满足特定查找算法。

**索引是在存储引擎中实现的**，存储引擎可以定义每个表的**最大索引数和最大索引长度**。

### 1.2.2 索引优点

1. 降低数据库的 IO 成本
2. 通过创建唯一索引，可以保证数据表中每一行**数据的唯一性**
3. 可以加速表和表之间的连接
4. 可以显著**减少查询中分组和排序的时间**

### 1.2.3 索引缺点

1. 创建和维护索引需要耗费时间。
2. 索引需要占用磁盘空间，需要存储在磁盘上。
3. 虽然索引可以大大提高查询速度，但是会**降低更新表的速度**。

## 1.3 InnoDB 中索引的推演

![image-20220301095843796](image-20220301095843796.png)

### 1.3.1 没有索引的查找

#### 1.3.1.1 在一个页中查找

假设目前表中的记录比较少，都存放在一个页中，在查找记录的时候可以根据搜索条件的不同分为两种情况：

1. 以主键为搜索条件：可以在页目录中使用二分法快速定位对应的槽，然后再遍历该槽对应分组中的记录来快速找到指定记录
2. 以其他列为搜索条件：因为在数据页中没有对非主键列建立所谓的页目录。我们只能从最小纪录开始一次遍历单链表中的每条记录，然后进行对比。很明显效率低于前者。

#### 1.3.1.2 在很多页中查找

在很多页中查找记录应该分为两个步骤：

1. 定位到记录所在的页
2. 从所在的页中查找相关记录

在没有索引的情况下，由于我们不能快速定位到记录对应的页，我们只能从第一个页沿着双向链表一直往下找，然后在每一个页上查找所有的记录（使用在一个页中查找的方法）。这种方法是十分耗时的。

### 1.3.2 设计索引

![image-20220301100808156](image-20220301100808156.png)

![image-20220301100910141](image-20220301100910141.png)

![image-20220301101015332](image-20220301101015332.png)

#### 1.3.2.1 一个简单的索引设计方案

![image-20220301101242411](image-20220301101242411.png)

![image-20220301101311161](image-20220301101311161.png)

![image-20220301101401777](image-20220301101401777.png)

![image-20220301101507286](image-20220301101507286.png)

![image-20220301101550395](image-20220301101550395.png)

![image-20220301101739193](image-20220301101739193.png)

至此，针对数据页做的简易目录就搞定了，这个目录有一个别名，称为**索引**。

#### 1.3.2.2 InnoDB 中的索引方案

​	迭代 1 次：目录项记录的页

![image-20220301102603357](image-20220301102603357.png)

![image-20220301102844857](image-20220301102844857.png)

​	迭代 2 次：多个目录项记录的页

![image-20220301103241634](image-20220301103241634.png)

​	迭代 3 次：目录项记录页的目录页

![image-20220301103510343](image-20220301103510343.png)

![image-20220301103655529](image-20220301103655529.png)

​	B+Tree 

![image-20220301103821529](image-20220301103821529.png)

通常情况下，我们用到的 B+树不会超过 4 层。（**树的层次越低，IO 的次数就会越少，所以针对 B+树，树越扁越好**）

所以我们通过主键值去查找某条记录最多只会进行 4 次 IO！！即进行四个页面内的查找。而在每个页面内又由所谓的**页目录**，所以在页面内部我们也可以通过二分法快速定位记录。

### 1.3.3 常见索引概念

索引按照物理实现方式，索引可以分为两种：聚簇索引和非聚簇索引。非聚簇索引也称为二级索引或辅助索引。

#### 1.3.3.1 聚簇索引

聚簇索引并不是一种单独的索引类型，而是**一种数据存储方式**（所有的用户记录都存储在了叶子节点）。也就是所谓的**索引即数据，数据即索引**。

特点：

1. 使用记录主键值的大小进行记录和页的排序，包括三个含义：
   - **页内**的记录是按照主键的大小顺序排成的一个**单向链表**。
   - 各个存放**用户记录的页**也是根据页中用户记录的主键大小顺序排成一个**双向链表**。
   - 存放**目录项记录的页**分为不同层次，在同一层次中的页也是根据页中目录项记录的主键大小顺序排成的一个**双向链表**。
2. B+树的**叶子结点**存储的是完整的用户记录。所谓的完整的用户记录，就是指这个记录中存储了所有列的值（即所有的用户记录）

我们把具有这两种特性的 B+树称为**聚簇索引**，所有完整的用户记录都会存放在这个聚簇索引的叶子节点处。

这种聚簇索引并不需要我们在 MySQL 中显示使用 `INDEX` 语句创建，InnoBD 存储引擎会自动为我们创建聚簇索引。

优点：

- 数据访问更快
- 对于主键的排序查找和范围查找速度非常快
- 节省了大量的 IO 操作

缺点：

- 插入速度严重依赖于插入顺序，**我们一般都会定义一个自增的 ID 列作为主键**
- 更新主键的代价很高，**一般定义主键为不可更新的**
- 二级索引访问需要两次索引查找，第一次找到主键值，第二次根据主键值找到行数据

限制：

- 只有 InnoBD 支持，MyISAM 不支持聚簇索引。
- 每一个**表只能有一个聚簇索引**，一般就是该表的主键。
- 如果没有定义主键，InnoDB 会选择非空唯一的索引代替，如果没有这种列，InnoDB 会隐式定义一个主键来作为聚簇索引。
- 为了充分利用聚簇索引的特性，InnoDB 表的主键**尽量选用有序的顺序 id**，而不建议用无序的 id，比如 UUID 等随机字符序列。

#### 1.3.3.2 二级索引（辅助索引、非聚簇索引）

![image-20220301105900539](image-20220301105900539.png)

![image-20220301110152760](image-20220301110152760.png)

![image-20220301110312166](image-20220301110312166.png)

聚簇索引与非聚簇索引的原理不同，在使用上也有一些区别：

1. 聚簇索引的叶子结点存储的就是**数据记录**，非聚簇索引的叶子结点存储的是数据位置。非聚簇索引不会影响数据表的物理存储顺序。
2. 一个表**只能有一个聚簇索引**，因为只能有一种排序存储方式，但可以有多个非聚簇索引，也就是多个索引目录提供数据检索。
3. 使用聚簇索引的时候，**数据的查询效率高**，但是如果对数据进行插入删除更新等操作，效率会比非聚簇索引低。

#####  1.3.3.3 联合索引

我们也可以同时以多个列的大小作为排序规则，也就是同时为多个列建立索引。比方说我们想让 B+树按照 c2 和 c3 列的大小进行排序，这个包含两层含义：

- 先把各个记录和页按照 c2 列进行排序
- 在记录的 c2 页相同的情况下，采用 c3 列进行排序

![image-20220301154918851](image-20220301154918851.png)

联合索引与分别为 c2、c3 列分别建立索引是不同的：

- 建立联合索引只会建立如上图一样的一颗 B+Tree
- 为 c2 和 c3 列分别建立索引会分别以 c2 和 c3 列的大小为排序规则建立两颗 B+Tree

### 1.3.4 InnoDB 的 B+树索引的注意事项

#### 1.3.4.1 根页面位置万年不动

实际上 B+树的形成过程是这样的：

- 每当为某个表创建一个 B+树索引（聚簇索引默认就存在）的时候，就会为这个索引创建一个**根节点页面**。最开始表中没有数据的时候，每个 B+树索引对应的**根节点**中既没有用户记录，也没有目录项记录。
- 随后向表中插入用户记录时，先把用户记录存储到这个**根节点**中。
- 当根节点中可用的空间用完后，根节点中所有的记录复制到一个新分配的页，然后对这个新页进行**页分裂**的操作。分成页 a 和页 b，之后新插入的记录根据值被分配在页 a 或者页 b 中，**而根节点升级为存储目录项记录的页**。

这个过程需要特别注意的是：**一个 B+树自诞生后，便不会在移动**。这样只要我们对某个表建立一个索引，那么根节点便会被记录在一个地方，之后凡是 InnoDB 存储引擎需要用到这个索引的时候，都会从这个固定的地方取出根节点的页号，来访问这个索引。

#### 1.3.4.2 内节点中目录项记录的唯一性

![image-20220301161230542](image-20220301161230542.png)

![image-20220301161302852](image-20220301161302852.png)

![image-20220301161449112](image-20220301161449112.png)

![image-20220301161534477](image-20220301161534477.png)

#### 1.3.4.3 一个页面最少存储两条记录

InnoDB 的一个数据页至少可以存储两条记录。防止每个数据页存的很少，但是目录层级会非常非常多的情况发生。

## 1.4 MyISAM 中的索引方案

![image-20220301162016026](image-20220301162016026.png)

这里的 B-Tree 就是 B+树。MyISAM 虽然使用 B+树作为索引结构，但是叶子结点中 data 域存放的是**数据记录的地址**。

### 1.4.1 MyISAM 索引的原理

在 InnoDB 中，**索引即数据**，索引的 B+树就把用户的所有完整记录都包含了。而 MyISAM 的索引方案虽然也使用树形结构，但是却**将索引和数据分开存储**。

- 表中的记录**按照记录的插入顺序**单独存储在一个文件中，称为数据文件。这个文件不划分数据页，有多少记录就往这个文件中塞多少条数据。插入时没有可以按照主键大小排序，所以不能在这些数据上使用二分法查找。
- 使用 MyISAM 存储引擎的表会把索引信息存储在另外一个称为**索引文件**的文件中，**MyISAM 会单独为表的主键创建一个索引**，只不过在索引的叶子节点中存储的不是完整的用户记录，而是**主键值+数据记录地址**的组合。

![image-20220301162726939](image-20220301162726939.png)

![image-20220301163232755](image-20220301163232755.png)

因此，MyISAM 中索引检索的算法为：首先按照 B+树搜索算法搜索索引，如果 Key 存在，取出 data 域中的值，然后根据 data 域中的值作为地址，读取相应的数据记录。

### 1.4.1 MyISAM 与 InnoDB 对比

**MyISAM 的索引方式都是非聚簇的，而 InnoDB 中一定包含一个聚簇索引。二者的区别如下：**

- MyISAM 一定需要一次回表操作，意味着 MyISAM 建立的索引全都相当于二级索引。
- InnoDB 中数据文件就是索引文件，而 MyISAM 数据文件和索引文件是分离的，索引文件只包含对应的数据文件地址。
- InnoDB 非聚簇索引 data 域存放的是**对应记录主键的值**，而 MyISAM 存储的是**索引记录的地址**。InnoDB 所有非聚簇索引都引用主键作为 data 域。
- MyISAM 的回表操作十分快速，InnoDB 如果发生回表操作，速度没有 MyISAM 快。
- InnoDB 要求表**必须有主键**（MyISAM 可以没有）。如果没有显式指定，MySQL 系统会自动选择一个可以非空且唯一表示数据记录的列作为主键。如果不存在这种列，MySQL 会自动为 InnoDB 生成一个隐含字段作为主键。这个字段长度为 6 个字节，类型为长整型。

![image-20220301164115854](image-20220301164115854.png)

## 1.5 索引的代价

索引会在空间和时间上造成消耗：

- 空间代价：一个页默认会占用 16KB 的存储空间，每个索引需要一个 B+树，每一个 B+树的每一个节点都是一个数据页。
- 时间代价：每次对数据进行增删改时，都需要去修改各个 B+树索引。这些操作需要额外的时间进行**记录移位、页面分裂、页面回收**等操作来维护好节点和记录的排序。

## 1.6 MySQL 数据结构选择的合理性

MySQL 选择索引的关键标准是：**磁盘的 I/O 次数**

数据库索引是存储在外部磁盘上的，我们利用索引查询时，不可能把整个索引加载到内存，只能逐一加载。那么衡量 MySQL 查询效率的标准就是磁盘 IO 次数。

### 1.6.1 全表遍历

我都懒得说，性能能好吗？

### 1.6.2 Hash 结构

![image-20220301165035107](image-20220301165035107.png)

从效率上来说，Hash 比 B+树还要快。

![image-20220301165320473](image-20220301165320473.png)

Hash 结构效率高，那为什么索引结构要设计成树形呢？

1. Hash 搜索仅能满足等于、不等于和 `IN` 查询。如果进行**范围查询**，哈希索引时间会退化为 O (n)，树形结构还是 O (logn)
2. Hash 索引的数据存储是**没有顺序的**，在 `ORDER BY` 的情况下，使用 Hash 索引还需要对数据重新排序
3. 对于联合索引的情况，Hash 值是将联合索引键合并后一起计算的，无法对单独的一个键或者几个索引键查询。
4. 对于等值查询来说，Hash 通常的效率会更高，但是**索引列的重复值如果很多，效率就会降低**。因为发生 Hash 冲突时，需要遍历桶中的行指针来进行比较，非常耗时。**所以 Hash 索引通常不会用到重复值多的列上。**

![image-20220301170256383](image-20220301170256383.png)

![image-20220301170513621](image-20220301170513621.png)

- 我们可以通过 innodb_adaptive_hash_index 变量来查看是否开启了自适应 Hash，比如：

  ```mysql
  mysql> show variables like '%adaptive_hash_index';
  ```

### 1.6.3 二叉搜索树

如果选用二叉树作为索引结构，那么磁盘 IO 和索引树的高度是相关的。

二叉搜索树如果在极端情况下，可能会退化为一条链表，查找数据的时间复杂度就变成 O (n)。为了提高查询效率，就需要减少磁盘 IO 次数，就需要**尽量降低树的高度**。

### 1.6.4 AVL 树（平衡二叉搜索树）

它是一颗空树或它的左右两个子树的高度差的绝对值不超过 1，并且左右两个子树都是一颗平衡二叉树。

常见的平衡二叉树有很多种：平衡二叉搜索树、红黑树、树堆、伸展树等。

针对这样的数据，如果我们把二叉树改成 M 叉树呢？当树的分叉树 M 变大的时候，树的整体高度就会变低，需要的磁盘 I/O 操作次数就会减少。所以，树越“矮胖”，就越好。

### 1.6.5 B-Tree

B 树也叫做**多路平衡查找树**。

![image-20220301171431553](image-20220301171431553.png)

![image-20220301171642703](image-20220301171642703.png)

同时，如果使用 B 树作为索引的存储结构，那么非叶子结点里面是会包含数据的完整信息的（就不是把所有的数据信息全都存储在叶子结点中了，而是在整个树的所有节点都有数据的信息）

**小结：**

- B 树在插入和删除节点如果导致树不平衡，就会自动的调整节点的位置来保持树的自平衡。
- 关键字集合分布在整棵树中，即叶子结点和非叶子节点都存放数据，搜索可能会在非叶子结点结束。
- 其搜索性能等价于在关键字全集内做一次二分查找。

### 1.6.6 B+树

B+树是改进的 B 树，也是一种多路搜索树。**B+树更适合文件索引系统**

B+树和 B 树的区别：

- 有 k 个孩子的节点就有 k 个关键字。也就是孩子数量 = 关键字数。而 B 树中，孩子数量 = 关键字数 + 1
- 非叶子节点的关键字也会同时存放在子节点中，并且是在子节点中所有关键字的最大或最小
- 非叶子节点仅用于索引，不保存数据记录，和记录有关的信息都放在叶子结点中。在 B 树中，**非叶子结点既保存索引，也保存数据记录。**
- 所有关键字都在叶子节点中出现，叶子节点构成一个有序链表，而且叶子节点本身按照关键字的大小从小到大顺序连接。

B+树的中间节点不直接存储数据的好处：

![image-20220316111205963](image-20220316111205963.png)

**B 树和 B+树都可以作为索引的数据结构，在 MySQL 中使用的是 B+树。他们各自有各自的应用场景，不能完全替代。**

![image-20220301172727147](image-20220301172727147.png)

![image-20220301172822980](image-20220301172822980.png)

### 1.6.7 R 树

![image-20220301172932170](image-20220301172932170.png)



# InnoDB 数据存储结构

## 2.1 数据库的存储结构：页

我们重点讲解 InnoBD 存储引擎的数据存储结构。

### 2.1.1 磁盘与内存交互基本单位：页

InnoBD 把数据划分为若干个页，每个页的默认大小是 16KB。**以页作为磁盘和内存之间交互的基本单位**。也就是说，一次最少从磁盘中读取 16KB 的内容到内存中，一次最少把 16KB 的内容刷新到磁盘中。

**数据库中，无论读一行，还是读多行，都是将这些行所在的页进行加载**。数据库管理存储空间的基本单位是页，数据库 I/O 操作的最小单位是页。一个页中可以存储多个行记录。

### 2.1.2 页结构概述

页与页之间**可以不在物理结构上相连**，只要通过**双向链表**相关联即可。每个数据页中的记录会按照主键值从小到大的顺序组成一个**单向链表**，每个数据页都会为存储在它里面的记录生成一个**页目录**，在通过主键查找某条记录时就可以使用二分法快速定位到对应的槽，然后遍历该槽就可以快速找到指定记录。

### 2.1.3 页的大小

MySQL 默认页的大小是 16KB。SQL Server 中页的大小是 8KB。Oracle 中用块来代表页的概念。

### 2.1.4 页的上层结构

![image-20220301194911857](image-20220301194911857.png)

![image-20220301195013837](image-20220301195013837.png)

## 2.2 页的内部结构

页如果按照类型划分，常见有：数据页（保存 B+树节点）、系统页、Undo 页和事务数据页等。数据页是我们最常使用的页（保存 B+树节点）

数据页的 16KB 大小的存储空间被划分为七个部分：

![image-20220301195457975](image-20220301195457975.png)

 ![image-20220301195614947](image-20220301195614947.png)

### 2.2.1 第一部分：File Header 和 File Trailer（文件头部和文件尾部）

#### 2.2.1.1 File Header

![image-20220301200027173](image-20220301200027173.png)

其中**页号可以用来唯一确定一个页**。

`FIL_PAGE_TYPE` 可以表示页的类型：

![image-20220301200216998](image-20220301200216998.png)

`FIL_PAGE_PREV` 和 `FIL_PAGE_NEXT`

![image-20220301200309584](image-20220301200309584.png)

文件头部中校验和 `FIL_PAGE_SPACE_OR_CHKSUM` 的作用：

![image-20220301200413085](image-20220301200413085.png)

`FIL_PAGE_LSN`

页面被最后修改时对应的日志序列位置（英文名是：Log Sequence Number）

#### 2.2.1.2 File Trailer

![image-20220301201110163](image-20220301201110163.png)

### 2.2.2 第二部分：User Records（用户记录）、最大最小记录、Free Space（空闲空间）

第二部分是记录部分，页的主要作用就是存储记录。所以 User Records（用户记录）、最大最小记录占用了页结构的主要空间。

#### 2.2.2.1 Free Space

![image-20220301201323286](image-20220301201323286.png)

#### 2.2.2.2 User Records

![image-20220301201352601](image-20220301201352601.png)

```mysql
create table page_demo(
    c1 int,
    c2 int,
    c3 varchar(10000),
    primary key(c1)
)charset=ascii row_format=Compact;
```

以下为记录头信息的部分内容：

![image-20220301201623983](image-20220301201623983.png)

![image-20220301201736811](image-20220301201736811.png)

![image-20220301201925867](image-20220301201925867.png)

![image-20220301202501174](image-20220301202501174.png)

![image-20220301202601291](image-20220301202601291.png)

![image-20220301202634841](image-20220301202634841.png)

![image-20220301203103003](image-20220301203103003.png)

![image-20220301203203210](image-20220301203203210.png)

#### 2.2.2.3 案例展示

![image-20220301203405628](image-20220301203405628.png)

![image-20220301203524171](image-20220301203524171.png)

![image-20220301203644136](image-20220301203644136.png)

说明：

- 当数据页中存在多条被删除掉的记录时，这些记录的 next_record 属性将会把这些被删除掉的记录组成一个垃圾链表，以备之后重用这部分存储空间

#### 2.2.2.4 最大最小记录

![image-20220301202949013](image-20220301202949013.png)

![image-20220301203039475](image-20220301203039475.png)

### 2.2.3 第三部分：Page Header 和 Page Directory（页目录和页面头部）

#### 2.2.3.1 Page Directory

![image-20220301204409316](image-20220301204409316.png) 

![image-20220301204535072](image-20220301204535072.png)

**总结：**每个组中包含 4-8 条记录，其中第一组只有一条记录（是整页的最小记录），最后一组有 1-8 条记录（最大记录一定在最后一组）。**每个组有一个对应的槽，作用是用来查找记录到底在哪个组的时候可以使用二分查找！**槽中保留的是对应组中的最大记录值对应的偏移量。组内的元素也是按照顺序划分的，所以每组的最后一个记录（也就是记录的最大值）的 `n_owned` 字段记录的内容是该组一共有多少条记录（不包含已经被标记为删除的记录）。除了每组的最后一个记录外，其余记录的 `n_owned` 字段为 0

![image-20220301204951341](image-20220301204951341.png)

![image-20220301205035482](image-20220301205035482.png)

![image-20220301205210220](image-20220301205210220.png)

![image-20220301205236320](image-20220301205236320.png)

![image-20220317084404993](image-20220317084404993.png)

**页目录结构下如何快速查找记录：**

- 首先根据 B+树的性质，确定记录在哪一页中，确定对应的**页面**。
- 确定了页之后，我们先在**槽中进行二分查找**，找到对应的槽对应的组。
- 由于槽中的元素是顺序递增的，而每组的元素不超过 8 个。这时我们使用**顺序查找找到对应的记录**即可。

#### 2.2.3.2 Page Header

![image-20220301205821586](image-20220301205821586.png)

注意：

- 所有被标记为删除的记录，会连接起来构成一个单链表。通过页面头部里面的 `PAGE_FREE` 字段，可以查找到所有被标记删除的记录。
- `PAGE_DIRECTION` ：假如新插入的一条记录的主键值比上一条记录的主键值大，我们说这条记录的插入方向是右边，反之是左边。该字段用来表示最后一条记录插入方向的状态。
- `PAGE_N_DIRECTION` ：假设连续几次插入新纪录的方向都是一致的，InnoDB 会把沿着同一个方向插入记录的条数记录下来，用这个字段来表示。如果最后一条记录插入方向改变的话，这个状态值就清零。

### 2.2.4 从数据页角度看 B+树如何查询

![image-20220302091526053](image-20220302091526053.png)

## 2.3 InnoBD 行格式（或记录格式）

InnoDB 存储引擎设计了四种不同类型的行格式：`Compact、Redundant、Dynamic、Compressed`

MySQL5.7 和 8.0 中的默认行格式是 `Dynamic`

![image-20220302091849021](image-20220302091849021.png)

### 2.3.1 `Compact` 行格式

在 MySQL5.1 中，默认设置为 Compact 行格式。一条完整的记录其实可以被分为记录的额外信息和记录的真实数据两大部分。

![image-20220302092118310](image-20220302092118310.png)

#### 2.3.1.1 变长字段长度列表

![image-20220302092336578](image-20220302092336578.png)

![image-20220317091820995](image-20220317091820995.png)

如果字段为 NULL，则变长字段长度列表中不会进行记录。

#### 2.3.1.2 NULL 值列表

![image-20220302092531536](image-20220302092531536.png)

同理，NULL 值列表中存储的 NULL 值位置和字段顺序也是反过来的，如下所示：（col4 在第一个位置，而 col1 在最后一个位置）

![image-20220302092800991](image-20220302092800991.png)

为什么图中 col2 不存在呢？因为**创建数据库表的时候如果定义了字段为 `NOT NULL` 属性或者为主键（主键一定不为空），就不需要在 NULL 值列表里面在记录一次了**。

#### 2.3.1.3 记录头信息

在 2.2.2.2 部分中有详细讲解。

#### 2.3.1.4 记录的真实数据（9.2.5 小节详解）

![image-20220302093634199](image-20220302093634199.png)

### 2.3.2 `Dynamic和Compressed` 行格式

#### 2.3.2.1 行溢出

InnoDB 存储引擎可以将一条记录中的某些数据存储在真正的数据页面之外。

![image-20220302095930258](image-20220302095930258.png)

该记录的情况下，最多能记录 65532 个字节的 `VARCHAR`，因为额外信息的部分为：**2 个字节的变长字段长度和 1 个字节的 NULL 值标识**。

![image-20220302100048491](image-20220302100048491.png)

![image-20220302100236526](image-20220302100236526.png)

#### 2.3.2.2 `Dynamic和Compressed` 行格式

在 MySQL8.0 中，默认行格式就是 `Dynamic`，`Dynamic` 和 `Compressed` 以及 `Compact` 格式十分类似，**只不过在处理行溢出时有一定分歧**：

![image-20220302100422806](image-20220302100422806.png)

### 2.3.3 `Redundant` 行格式

`Redundant` 是 MySQL5.0 版本之前 InnoDB 的行记录存储方式。后续 MySQL 支持 `Redundant` 是为了兼容之前版本的页格式。

![image-20220302100841854](image-20220302100841854.png)

#### 2.3.3.1 字段长度偏移列表

![image-20220302101100578](image-20220302101100578.png)

同时，也由于他记录了所有字段的长度（字段的长度可以直接通过偏移列表之间的相减从而得到），所以就不需要 NULL 值列表。

#### 2.3.3.2 记录头信息

![image-20220302101503158](image-20220302101503158.png)

![image-20220302101403704](image-20220302101403704.png)

## 2.4 区、段与碎片区

### 2.4.1 为什么要有区？

如果以页为单位来分配存储空间的话，双向链表两个相邻的页在物理磁盘上可能离得非常远。这样会变成**随机 I/O**，所以我们应该尽量让链表中相邻的页的物理位置也相邻，这样进行查询就是所谓的**顺序 I/O**。

引入区的概念，一个区就是物理位置上连续的 64 个页。一个区的大小是 1MB。在表数据量大的时候，为某个索引分配空间就不在按照页为单位分配，而是**按照区为单位分配**。相当于区中的页是连续的，但是区之间不是连续的，还是通过双向链表进行连接。甚至在表中数据特别多的时候，可以一次性分配多个连续的区。

### 2.4.2 为什么要有段？

区的概念仅仅是区别了页，防止页之间物理距离过远而产生的随机 I/O。但是区中包含的页可能是叶子节点页和非叶子节点页，我们需要找到的仅仅是叶子节点页，如果某一个区中的叶子结点页很少，那效率还是会大打折扣。

段的出现是为了区分 B+树中的叶子节点和非叶子节点。也就是说，叶子节点有自己独有的区，非叶子节点也有自己独有的区。存放叶子节点的区的集合就是一个段，存放非叶子节点的区的集合也算一个段。也就是说，**一个索引会生成两个段，一个叶子节点段，一个非叶子节点段。**

除了索引的叶子结点段和非叶子节点段。InnoDB 中还有为存储一些特殊数据而定义的段：数据段、索引段、回滚段。数据段为 B+树的叶子节点，索引段即为 B+树的非叶子节点。

段其实不对应表空间某一个连续的物理区域，而是逻辑上的概念，**由若干个零散的页面以及一些完整的区组成**。

### 2.4.3 为什么要有碎片区？

默认情况下，一个 InnoDB 存储引擎的表只有一个聚簇索引，一个索引生成两个段，而段以区为单位。那如果只存了几条记录的小表也需要这么大的存储空间吗？

为了考虑以完整的区为单位分配给某个段对于数据量较小的表太浪费存储空间的情况，InnoDB 提出来一个**碎片区（fragment）**的概念。在一个碎片区中，并不是所有的页都是为了存储同一个段而存在的，而是碎片区中的页可以用于不同的目的，有些页用于段 A，有些用于段 B。**碎片区只属于表空间，不属于任何一个段。**

所以为某个段分配存储空间的策略是这样的：

- 在刚开始向表中插入数据时，段是从某个碎片区以单个页面为单位来分配存储空间的。
- 当某个段已经占用了**32 个碎片区**页面之后，就会申请以完整的区为单位来分配存储空间。

所以现在的段不能仅定义为某些区的集合，**更精确应该是某些零散的页面以及一些完整的区**。

### 2.4.4 区的分类

区大体上有四种类型：

- 空闲区：现在还没有用到这个区的任何页面。
- 有剩余空间的碎片区：表示碎片区中还有可用的页面。
- 没有剩余空间的碎片区：表示碎片区中的所有页面都被使用，没有空闲页面。
- 附属于某个段的区：每一个索引都可以分为叶子节点段和非叶子节点段。

前三种区都是只属于表空间，最后一个区是附属于某个段的。

## 2.5 表空间

表空间可以看做 InnoDB 存储引擎逻辑结构的最高层，所有数据都存放在表空间中。

表空间是一个**逻辑容器**，表空间存储的对象是段，在一个表空间中可以有一个或多个段，但是一个段只能属于一个表空间。表空间数据库由一个或多个表空间组成，表空间从管理上可以划分为 `系统表空间`（System tablespace）、`独立表空间`（File-per-table tablespace）、`撤销表空间`（Undo tablespace）和 `临时表空间`（Temporary tablespace）等。

### 2.5.1 独立表空间

独立表空间，即每张表有一个独立的表空间，也就是数据和索引信息都会保存在自己的表空间中。独立的表空间可以在不同的数据库之间进行迁移。 

![image-20220317150804451](image-20220317150804451.png)

独立表空间由段、区、页组成。前面已经讲过了。

![image-20220317151659669](image-20220317151659669.png)

查看 InnoDB 的表空间类型：

```mysql
show variables like 'innodb_file_per_table';
```



### 2.5.2 系统表空间

系统表空间和独立表空间基本类似，只不过整个 MySQL 进程只有一个系统表空间，系统表空间中会额外记录一些有关整个系统信息的页面。

![image-20220302105048262](image-20220302105048262.png)

![image-20220302105151680](image-20220302105151680.png)

用户是不能直接访问这些 InnoDB 的内部系统表。不过考虑到查看这些表的内容可能有助于大家分析问题，所以在系统数据库 `information_schema` 中提供了一些以 `innodb_sys` 开头的表

（但是这个数据库中的表也不是真正的内部系统表，而是在存储引擎启动时读取这些以 SYS 开头的系统表，然后填充到这些以 INNODB_SYS 开头的表中。以 INNODB_SYS 开头的表和以 SYS 开头的表的字段并不完全一样，仅供参考）



# 索引的创建与设计原则

## 3.1 索引的声明与使用

### 3.1.1 索引的分类

![image-20220302110121047](image-20220302110121047.png)

#### 3.1.1.1 普通索引

在创建普通索引时，不附加任何限制条件，只是用于提高查询效率，这类索引可以创建在任何数据类型中。建立索引之后，可以通过索引进行查询。使用 `WHERE` 字句查找该字段就可以加快查找效率。

#### 3.1.1.2 唯一性索引

使用 `UNIQUE` 参数可以设置索引为唯一性索引，限制该索引值必须是唯一的，但是允许有空值。**一张数据表中可以有多个唯一索引**。

#### 3.1.1.3 主键索引

主键索引就是一种特殊的唯一性索引，加上了不为空的约束，也就是 `NOT NULL + UNIQUE`，一张表最多只有一个主键索引。主键索引也就是我们说的聚簇索引。

#### 3.1.1.4 全文索引

全文索引是目前**搜索引擎**使用的一种关键技术，它能够利用**分词技术**等多种算法只能分析出文本文字中关键字的频率和重要性。关系型数据库应对全文索引的需求已经力不从心，逐渐被 solr、ElasticSearch 等专门的搜索引擎所替代。

全文索引典型两种类型：自然语言的全文索引和布尔全文索引。

#### 3.1.1.5 空间索引

使用参数 `SPATIAL` 可以设置索引为空间索引。空间索引只能建立在空间数据类型上，这样可以提高系统获取空间数据的效率。目前只有 MyISAM 存储引擎支持空间检索，而且索引的字段不能为空值。

![image-20220302110936906](image-20220302110936906.png)

### 3.1.2 创建索引

MySQL 支持多种方法创建索引：

- 在创建表的定义语句 `CREATE TABLE` 中指定索引列
- 使用 `ALTER TABLE` 语句在存在的表上创建索引
- 使用 `CREATE INDEX` 语句在已存在的表上添加索引

#### 3.1.2.1 创建表的时候创建索引

```sql
# 隐式的方式创建索引：主键约束、外键约束以及唯一性约束都会自动创建相关的索引
CREATE TABLE dept(
    dept_id INT PRIMARY KEY AUTO_INCREMENT,
    dept_name VARCHAR(20)
);

CREATE TABLE emp(
    emp_id INT PRIMARY KEY AUTO_INCREMENT,
    emp_name VARCHAR(20) UNIQUE,
    dept_id INT,
    CONSTRAINT emp_dept_id_fk FOREIGN KEY(dept_id) REFERENCES dept(dept_id)
)

# 显式的创建索引
CREATE TABLE book(
    book_id INT,
    book_name VARCHAR(100),
    AUTHORS VARCHAR(100),
    info VARCHAR(100),
    COMMENT VARCHAR(100),
    year_publication YEAR,
    # 下面声明索引
    INDEX idx_bname(book_name)
);

# 通过命令查看索引
# 方式1：
SHOW CREATE TABLE book;
# 方式2：
SHOW INDEX FROM book;

# 显式创建唯一索引
CREATE TABLE book1(
    book_id INT,
    book_name VARCHAR(100),
    AUTHORS VARCHAR(100),
    info VARCHAR(100),
    COMMENT VARCHAR(100),
    year_publication YEAR,
    # 下面声明唯一索引,对COMMENT字段设置为唯一索引，同时创建了唯一性索引之后，该字段不能重复
    UNIQUE INDEX uk_idx_cmt(COMMENT)
);
# 显式创建主键索引，即通过定义主键约束的方式定义主键索引。
CREATE TABLE book2(
    # 通过显式定义主键之后，就可以创建主键索引，因为只要有主键，主键索引会自动创建
    book_id INT PRIMARY KEY,
    book_name VARCHAR(100),
    AUTHORS VARCHAR(100),
    info VARCHAR(100),
    COMMENT VARCHAR(100),
    year_publication YEAR
);
# 通过删除主键约束的方式删除主键索引
ALTER TABLE book2
DROP PRIMARY KEY;
# 显式创建单列索引（前面创建的索引都是单列索引，不再赘述）

# 显式创建联合索引
CREATE TABLE book(
    book_id INT,
    book_name VARCHAR(100),
    AUTHORS VARCHAR(100),
    info VARCHAR(100),
    COMMENT VARCHAR(100),
    year_publication YEAR,
    # 下面声明联合索引,联合索引的顺序非常重要，确定先按照哪一个字段来进行索引查找或者设置索引，一般把经常检索的字段放在第一位
    INDEX mul_bid_bname_info(book_id,book_name,info)
);

# 创建全文索引
CREATE TABLE test4(
    id INT NOT NULL,
    NAME CHAR(30) NOT NULL,
    age INT NOT NULL,
    info VARCHAR(255),
    # 下面创建全文索引，（50）表示这个字段的前50个字符来做索引，保证索引不会特别大，可以不指定，代表使用这个字段的全部内容
    FULLTEXT INDEX futxt_idx_info(info(50))
)
```

![image-20220302195531630](image-20220302195531630.png)

注意：

- 声明有唯一性索引的字段，在添加数据时，要保证唯一性，但是可以添加 NULL

- 全文索引只能为`CHAR、VARCHAR、TEXT`列创建索引。索引总是对整个列进行，不支持局部索引。

- 全文检索查找时不使用`LIKE`使用`MATCH + AGAINST`来代替：

  ![image-20220302202008910](image-20220302202008910.png)

#### 3.1.2.2 在已经存在的表上创建索引

```sql
# 表已经创建成功
CREATE TABLE book5(
    book_id INT,
    book_name VARCHAR(100),
    AUTHORS VARCHAR(100),
    info VARCHAR(100),
    COMMENT VARCHAR(100),
    year_publication YEAR,
);
# 方式1添加索引：
ALTER TABLE book5 ADD INDEX idx_cmt(COMMENT);
ALTER TABLE book5 ADD UNIQUE INDEX uk_idx_bname(book_name);
ALTER TABLE book5 ADD INDEX mut_bid_bname_info(book_id,book_name,info);

# 方式2添加索引
CREATE INDEX idx_cmt ON book5(COMMENT);
CREATE UNIQUE INDEX uk_idx_bname ON book5(book_name)
CREATE INDEX mut_bid_bname_info ON book5(book_id,book_name,info)
```

### 3.1.3 删除索引

```SQL
# 方式1：使用ALTER TABLE删除索引
ALTER TABLE book5 DROP INDEX idx_cmt；
# 添加AUTO_INCREMENT约束字段的唯一索引不能被删除。

# 方式2：使用DROP INDEX ON删除索引
DROP INDEX uk_idx_bname ON book5;
```

> 提示 1：添加 AUTO_INCREMENT 约束字段的唯一索引不能被删除。
>
> 提示 2：删除表中的列时，如果删除的列为索引的组成部分，则该列也会从索引中删除。如果组成索引的所有列都被删除，则整个索引将被删除。

## 3.2 MySQL8.0 索引新特性

### 3.2.1 支持降序索引

![image-20220302203350111](image-20220302203350111.png)

```sql
# 创建降序索引:联合索引按照a字段升序，b字段降序
CREATE TABLE ts1(
    a INT,
    b INT,
    INDEX idx_a_b(a ASC,b DESC)
);
```

### 3.2.2 隐藏索引

![image-20220302204606045](image-20220302204606045.png)

```sql
# 隐藏索引
# 创建表时，隐藏索引
CREATE TABLE book7(
    book_id INT,
    book_name VARCHAR(100),
    AUTHORS VARCHAR(100),
    info VARCHAR(100),
    COMMENT VARCHAR(100),
    year_publication YEAR,
    # 创建不可见的索引
    INDEX idx_cmt(COMMENT) invisible
);

# 创建表以后，创建隐藏索引
ALTER TABLE book7
ADD UNIQUE INDEX un_idx_bname(book_name) invisible;

# 修改索引的可见性
ALTER TABLE book7 ALTER INDEX idx_year_pub invisible; # 可见--->不可见
ALTER TABLE book7 ALTER INDEX idx_cmt visible; # 不可见--->可见
```

当索引被隐藏时，它的内容仍然是和正常索引一样实时更新的。如果一个索引需要长期被隐藏，那么可以将其删除，因为索引的存在会影响插入、更新和删除的性能。

**注意，这里的两个新特性都可以谈到索引优化的地方：**

- 首先可以使用降序索引来优化结构
- 其次可以使用隐藏索引，来判断索引是否必须，进一步优化索引。也可以对删除索引实现软删除，保证删除索引的合理性和安全性。

## 3.3 索引的设计原则

### 3.3.0 数据准备

详看 PDF 第 08 章

### 3.3.1 哪些情况适合添加索引

#### 3.3.1.1 字段的数值有唯一性约束

在数据表中，如果某个字段是唯一性的，就可以直接创建**唯一性索引**，或者**主键索引**。这样可以更快速地通过该索引来确定某条记录。

> 业务上具有唯一特性的字段。即使是组合字段，也必须建成唯一索引。（来源：Alibaba）
>
> 说明：不要以为唯一索引影响了 insert 速度，这个速度消耗可以忽略，但提高查找速度是明显的。
>

#### 3.3.1.2 频繁作为 `WHERE` 查询条件的字段

如果某个字段在 `SELECT` 语句的 `WHERE` 条件中经常被使用到，那么我们就需要给这个字段创建索引了。在数据量很大的情况下，创建普通索引会加速查询的过程。

#### 3.3.1.3 经常 `GROUP BY` 和 `ORDER BY` 的列

使用 `GROUP BY` 和 `ORDER BY` 对数据进行排序的时候，就需要**对分组或者排序的字段进行索引**。如果待排序的列有多个，可以在这些列上建立**联合索引**。

如果 `GROUP BY` 和 `ORDER BY` 同时在查询语句存在，我们**需要创建联合索引**，因为是先走 `GROUP BY` 语句，然后执行 `ORDER BY` 语句。如果分别建立索引，`ORDER BY` 的索引就失去了意义。同时，这个联合索引创建的顺序也是先按照 `GROUP BY` 语句的字段索引，再按照 `ORDER BY` 的字段索引，这样联合索引的效率是最高的（**顺从 SQL 语句的执行规则**）

#### 3.3.1.4 `UPDATE、DELETE` 的 `WHERE` 条件列

对数据按照某个条件进行查询后再进行 `UPDATE、DELETE` 的操作，如果对 `WHERE` 字段创建了索引，就能大幅提升效率。原理是我们需要先根据 `WHERE` 条件列检索出来这条记录，然后进行更新和删除。**如果进行更新的时候，更新的字段是非索引字段，提升的效率会更明显，这是因为非索引字段的更新不需要对索引进行维护。**

#### 3.3.1.5 `DISTINCT` 字段需要创建索引

针对 `DISTINCT` 字段，如果创建了索引，在去重时会提升查询的效率。同时，索引会对数据按照某种顺序进行排序，所以在去重的时候也会快很多。

#### 3.3.1.6 多表 `JOIN` 连接操作时，创建索引注意事项

首先，**连接表的数量尽量不要超过 3 张**，因为每增加一张表就相当于多一次嵌套循环。

其次，**对 `WHERE` 条件创建索引**，因为 `WHERE` 才是对数据条件的过滤。如果在数据量非常大的情况下，没有 `WHERE` 条件过滤非常可怕。<!--视频中测试 WHERE 条件有索引是 1ms，没有是 227ms，个人认为，起码对于同时存在 join 和 where 并且 where 条件走索引时，先执行 where，再 JOIN，否则，如果先 join，再 where，不可能在 1ms 完成查询。那么如果 join 可以走索引，where 也可以走索引，会先执行谁呢？如果 join 可以走索引，where 不能走索引，先执行谁呢？-->

最后，**对用于连接的字段创建索引**，并且该字段在多张表中**类型必须一致**。

#### 3.3.1.7 使用列的类型小的创建索引

这里的列的**类型大小**指的是该类型表示的数据范围大小。

数据类型越小，在查询时进行比较操作越快。数据类型越小，索引占用的存储空间就越少，在一个数据页内就可以放下更多的记录，减少磁盘 I/O 带来的损耗。**这个建议对于表的主键来说更加适用。**因为其他二级索引的节点处都会存储一份记录的主键值，如果主键使用更小的数据类型，意味着节省更多的存储空间和更高效的 I/O。

#### 3.3.1.8 使用字符串前缀创建索引

如果字符串很长，那么建立索引在 B+树中就有两个问题：

- B+树索引中的记录需要把该列的完整字符串存储起来，而且字符串很长，在索引中**占用的存储空间越大**。
- 如果 B+树索引中索引列存储的字符串很长，那在做字符串**比较时会占用很多时间**。

我们可以通过截取字段前面一部分内容建立索引，就叫做**前缀索引**，这样既节约空间，又减少了字符串的比较时间。

注意：使用索引列前缀的方式**无法支持使用索引排序**，只能使用文件排序。

![image-20220317181007197](image-20220317181007197.png)

#### 3.3.1.9 区分度高（散列性高）的列适合作为索引

列的基数指的是某一列中不重复数据的个数。在记录行数一定的情况下，列的基数越大，该列中的值越分散；列的基数越小，该列中的值越集中。最好为列的基数大的列建立索引。

可以使用公式 `select count(distinct a)/count(*) from t1` 计算区分度，越接近 1 越好，一般超过 33%就算是比较高效的索引了。

拓展：联合索引把区分度高（散列值高）的列放在前面

#### 3.3.1.10 使用最频繁的列放到联合索引的左侧

这样也可以较少的建立一些索引。同时，由于“最左前缀原则”，可以增加联合索引的使用率

#### 3.3.1.11 在多个字段都要创建索引的情况下，联合索引优于单值索引

同时，在多个字段都要创建索引的情况下，联合索引是优于多个单值索引的。

### 3.3.2 限制索引的数目

实际工作中，建议索引的数目不是越多越好，建议单张表的索引数量不超过 6 个：

- 每个索引需要占用额外的磁盘空间，索引越多，需要额外空间越大。
- 索引会影响 `INSERT、DELKETE、UPDATE` 语句的性能，因为表数据改变时，索引也会进行调整和更新。
- 优化器在选择如何优化查询时，会根据统一信息，**对每一个可以用到的索引进行评估**，以生成最好的执行计划。如果索引很多，会增加 MySQL 优化器生成执行计划时间，降低查询性能。

### 3.3.3 哪些情况不适合创建索引

1. 在 `WHERE` 中使用不到的字段不要添加索引
2. 数据量小的表最好不要使用索引
3. 有大量重复的列上不要建立索引
4. 避免对经常更新的表建立过多的索引
5. 不建议用无序的值作为索引
6. 删除不再使用或者很少使用的索引
7. 不要定义冗余或者重复的索引



# 性能分析工具的使用

## 4.1 数据库服务器的优化步骤

![image-20220303145401827](image-20220303145401827.png)

![image-20220303145510854](image-20220303145510854.png)

![image-20220303145722339](image-20220303145722339.png)

小结：

![image-20220303145853481](image-20220303145853481.png)

## 4.2 查看系统性能参数

在 MySQL 中，我们可以使用 `SHOW STATUS` 语句查询一些 MySQL 数据库服务器的**性能参数、执行频率**。

![image-20220303150014540](image-20220303150014540.png)

## 4.3 统计 SQL 的查询成本：`last_query_cost`

一条 SQL 查询语句在执行前需要指定查询计划，如果存在多种执行计划的话，MySQL 会计算每个执行计划所需要的成本。从中选择**成本最小**的一个作为最终执行的执行计划。

我们可以查看当前会话中的 `last_query_cost` 变量值来得到当前查询的成本。它是我们**评价一个查询的执行效率**的常用指标。这个查询成本对应的是**SQL 语句所需要读取的页的数量**。

![image-20220303151220204](image-20220303151220204.png)

## 4.4 定位执行慢的 SQL：慢查询日志

MySQL 的慢查询日志，用来记录在 MySQL 中**响应时间超过阈值**的语句，具体指运行时间超过 `long_query_time` 值的 SQL（默认值为 10）

它的主要作用是帮助我们发现那些执行时间特别长的 SQL 查询。然后针对性进行优化。收集到这些 SQL 后，可以结合 explain 进行全面分析

默认情况下，MySQL 数据库**没有开启慢查询日志**，需要我们手动设置。**如果不是为了调优，不建议开启该参数**，因为会造成性能影响。

### 4.4.1 开启慢查询日志参数

![image-20220303151854058](image-20220303151854058.png)

![image-20220303151952017](image-20220303151952017.png)

![image-20220317203716395](image-20220317203716395.png)

### 4.4.2 查看慢查询条目

查询当前系统中有多少条慢查询记录：

```SQL
SHOW GLOBAL STATUS LIKE '%Slow_queries%';
```

![image-20220317205420064](image-20220317205420064.png)

### 4.4.3 慢查询日志分析工具：mysqldumpslow

在生产环境中，MySQL 提供了日志分析工具 mysqldumpslow

![image-20220303153616050](image-20220303153616050.png)

### 4.4.4 关闭慢查询日志

MySQL 关闭慢查询日志有两种方法：

方法 1：永久性关闭

![image-20220303154039961](image-20220303154039961.png)

方法 2：临时性方式

![image-20220303154116929](image-20220303154116929.png)

### 4.4.5 删除慢查询日志

![image-20220303154428665](image-20220303154428665.png)

## 4.5 查看 SQL 执行成本：`SHOW PROFILE`

`SHOW PROFILE` 是 MySQL 提供的可以用来分析当前会话中 SQL 都做了什么、执行的资源消耗情况的工具，可用于 SQL 调优的测量。**默认情况下处于关闭状态**，并保存最近 15 次的运行结果。

![image-20220303155358247](image-20220303155358247.png)

![image-20220303155424927](image-20220303155424927.png)

我们也可以查看指定的 Query ID 的开销，比如 show profile for query 2 查询结果也是一样的。在 SHOW PROFILE 中我们可以查看不同部分的开销，比如 cpu、block. io 等。

![image-20220303155441153](image-20220303155441153.png)

![image-20220303155014093](image-20220303155014093.png)

## 4.6 查询分析语句：`EXPLAIN`

### 4.6.1 概述

可以使用 `EXPLAIN` 工具做针对性的分析查询语句。

MySQL 中有负责优化 `SELECT` 语句的优化器模块，它通过分析系统收集到的统计信息，提供它认为最优的执行计划（他认为最优的数据检索方式，但不见得是 DBA 认为是最优的，这部分最耗费时间）。

这个执行计划展示了接下来具体执行查询的方式，**我们可以使用 `EXPLAIN` 语句来帮助我们查看某个查询语句的执行计划**。

![image-20220303161211792](image-20220303161211792.png)

**2. 官网介绍**

https://dev.mysql.com/doc/refman/5.7/en/explain-output.html

![image-20220303161523818](image-20220303161523818.png)

### 4.6.2 基本语法

```sql
EXPLAIN SELECT select_options
或者
DESCRIBE SELECT select_options
# 如果我们想要看某个查询的执行计划的话，可以在具体的查询语句前面加一个EXPLAIN
```

注意：执行 `EXPLAIN` 时并没有真正的执行该后面的语句，因此可以安全的查看执行计划。

`EXPLAIN` 语句输出的各个列的作用如下：

![image-20220303161821399](image-20220303161821399.png)

### 4.6.3 `EXPLAIN` 各列的作用

#### 4.6.3.1 table

不论我们的查询语句有多复杂，里边儿**包含了多少个表**，到最后也是需要对每个表进行**单表访问**的，所以 MySQL 规定**`EXPLAIN` 语句输出的每条记录都对应着某个单表的访问方法**，该条记录的 table 列代表着该表的**表名**（有时不是真实的表名字，可能是简称）。

如果过程会产生临时表的情况，`EXPLAIN` 语句会记录临时表的这一行，table 字段同样也会记录这个表的表名。（比如 `UNION` 去重需要产生的临时表）

#### 4.6.3.2 id

在一个大的查询中，每个 `SELECT` 关键字都对应一个唯一的 id

**注意：查询优化器可能对设计子查询的查询语句进行重写，转变为多表查询的操作，导致子查询本来有多个 `SELECT`，经过优化之后 `SELECT` 数目变少，所以 id 数目发生变化**

同时，涉及产生临时表的情况，`EXPLAIN` 语句会记录临时表的这一行，但是 id 字段会写为 NULL（比如 `UNION` 去重需要产生的临时表）

小结：

- id 如果相同，可以认为是一组，从上往下去执行
- 在所有的组中，id 值越大，优先级越高，越先执行
- 关注点：id 号的每个号码，表示一趟独立的查询，一个 SQL 的查询趟数越少越好

#### 4.6.3.3 select_type

一条大的查询语句里面可以包含若干个 `SELECT` 关键字，**每个 `SELECT` 关键字代表一个小的查询语句**，而每个 `SELECT` 关键字的 `FROM` 字句中都可以包含若干张表，**每一张表都对应着执行计划输出中的一条记录**，对于同一个 `SELECT` 关键字中的表来说，他们的 id 是相同的。

MySQl 为每一个 `SELECT` 关键字代表的小查询都定义了一个称之为 select_type 的属性，意思是我们知道了某个小查询的 select_type，就知道了这个**小查询在整个大查询中扮演了一个什么角色**。

select_type 的取值如下：

![image-20220303164757998](image-20220303164757998.png)

- 查询语句中不包含 `UNOIN` 或者子查询的查询，都算 `SIMPLE` 类型

- 连接查询也算是 `SIMPLE` 类型

- 对于包含 `UNION` 或者 `UNION ALL` 或者子查询的大查询来说，它是由几个小查询组成的，其中最左边的查询的 `select_type` 值就是 `PRIMARY`（即 `UNION` 连接中最左边的和子查询中最外层查询的 `select_type` 值就是 `PRIMARY`）

- 对于包含 `UNION` 或者 `UNION ALL` 的大查询来说，它是由几个小查询组成的，其中除了最左边的那个小查询以外，其余的小查询的 `select_type` 值就是 `UNION`

- MySQL 选择使用临时表来完成 `UNION` 查询的去重工作，针对该临时表的查询的 select_type 就是 `UNION RESULT`

- 如果包含子查询的查询语句不能够转为对应的 `semi-join` 的形式（即子查询不能够优化成多表连接的形式），并且该子查询不是相关子查询，该子查询的第一个 `SELECT` 关键字代表的那个查询的 select_type 就是 `SUBQUERY`

- 如果包含子查询的查询语句不能够转为对应的 `semi-join` 的形式（即子查询不能够优化成多表连接的形式），并且该子查询是相关子查询，该子查询的第一个 `SELECT` 关键字代表的那个查询的 select_type 就是 `DEPENDENT SUBQUERY` (需要注意，select_type 为 `DEPENDENT SUBQUERY` 的查询可能会被执行多次)

  ```mysql
  select * from s1
  where key1 in (select key1 from s2 where s1.key2 = s2.key2) or key3 = 'a';
  #s1表的key2可能被多次传入内查询，所以说select_type为`DEPENDENT SUBQUERY`的查询可能会被执行多次
  ```

- 对于包含 `UNION` 或者 `UNION ALL` 的大查询来说，如果各个小查询都依赖于外层查询的话，那除了最左边的那个小查询以外，其余的小查询的 select_type 的值就是 `DEPENDENT UNION`

  ```mysql
  select * from s1
  where key1 in (select key1 from s2 where key1 = 'a' union select key1 from s1 where key1 = 'b');
  #很多时候，优化器会将IN改为exists执行，这时候，内查询就和外查询有相关性了
  ```

- 对于包含派生表的查询，该派生表对应的子查询的 select_type 就是 `DERIVED` (比如子查询声明在 `FROM` 字句中，这个子查询产生的表需要被对应的 `FROM` 字句使用，就是一个派生表)

  ```mysql
  select *
  from (select key1, count(*) as c from s1 group by key1) as derived_s1 where c > 1;
  ```

- 当查询优化器在执行包含子查询的语句时，选择将子查询物化之后与外层查询进行连接查询时，该子查询的 select_type 属性就是 `MATERIALIZED`（不在 `FROM` 字句中的子查询就会物化一个临时表）

  ```mysql
  select * from s1 where key1 in(select key1 from s2);#子查询被转为了物化表
  ```

#### 4.6.3.4 partitions

代表分区表中的命中情况，非分区表该项为 NULL。一般情况下我们的查询语句执行计划的 partitions 列的值都是 NULL

![image-20220303185145344](image-20220303185145344.png)

#### 4.6.3.5 type (重要)

执行计划的一条记录就代表着 MySQL 对某个表的**执行查询时的访问方法**，又称为访问类型，其中 type 列就表明了这个访问方法是啥。

完整的访问方法如下： `system ，const ， eq_ref ， ref ， fulltext ， ref_or_null ， index_merge ， unique_subquery ， index_subquery ， range ， index ， ALL `。**从前往后的访问方法性能依次递减。**

1. system：

   当表中**只有一条记录**并且**该表使用的存储引擎的统计数据是精确的**，比如 MyISAM、Memory，那么对该表的访问方法就是 system（InnoDB 的统计数据不是精确的，所以不可能是 system）

   ```sql
   CREATE TABLE t(i int) Engine=MyISAM;
   INSERT INTO t VALUES(1);
   # 下面这个EXPLAIN对应的type就是system
   EXPLAIN SELECT * FROM t;
   ```

2. const：

   当我们根据**主键或者唯一性约束二级索引**与**常数进行等值匹配**时，对单表的访问方法就是 const

   ```sql
   EXPLAIN SELECT * FROM s1 WHERE id = 10005;
   ```

3. eq_ref：

   在连接查询时，如果**被驱动表**是通过**主键或者唯一二级索引列等值匹配**的方式进行比较访问的（如果该主键或者唯一二级索引是联合索引的话，所有的索引列都必须进行等值比较），则对该被驱动表的访问方法就是 eq_ref

   ```sql
   # s1是驱动表，s2是被驱动表
   EXPLAIN SELECT * FROM s1 INNER JOIN s2 ON s1.id = s2.id;
   ```

4. ref：

   当通过普通的二级索引列与常量进行等值匹配来查询某个表，那么对该表的访问方法就可能是 ref (不能对字段做隐式转换，比如 `VARCHAR` 类型和数值类型比较)

   ```sql
   EXPLAIN SELECT * FROM s1 WHERE key1 = 'a';
   ```

5. ref_or_null：

   当对普通二级索引进行等值匹配查询，该索引列的值也可以是 NULL 值时，那么对该表的访问方法就可能是 ref_or_null（ref 的 NULL 值特殊情况）

   ```mysql
   EXPLAIN SELECT * FROM s1 WHERE key1 = 'a' OR key1 IS NULL;
   ```

6. index_merge：

   单表访问方法时在某些场景下可以使用 `Intersection、Union、Sort-Union` 这三种**索引合并的方式**来执行查询

   ```sql
   EXPLAIN SELECT * FROM s1 WHERE key1 = 'a' OR key3 = 'a';
   ```

7. unique_subquery：

   unique_subquery 是针对在一些包含 `IN` 子查询的查询语句中。如果查询优化器决定将 `IN` 子查询转换为 `EXISTS` 子查询，而且子查询可以使用到主键进行等值匹配的话，那么该子查询执行计划的 type 列的值就是 unique_subquery

   ```sql
   EXPLAIN SELECT * FROM s1 WHERE key2 IN (SELECT id FROM s2 where s1.key1 =
   s2.key1) OR key3 = 'a';
   ```

8. range：

   如果使用索引获取某些**范围区间**的记录，那么就可能使用到 range 访问方法

   ```sql
   EXPLAIN SELECT * FROM s1 WHERE key1 IN ('a', 'b', 'c');
   ```

9. index：

   当我们可以使用**索引覆盖**，但是需要扫描全部的索引记录时，该表的访问方法就是 index

   ```sql
   EXPLAIN SELECT key_part2 FROM s1 WHERE key_part3 = 'a';
   ```

   **要查询的字段和条件字段属于同一个联合索引**，直接根据联合索引进行遍历（按照 key_part1），虽然没有 key_part3 的索引，但是使用了对应的联合索引。（这里其实有一点违背最左原则，但是执行引擎为了防止回表操作，做出的优化）

10. all：

    最熟悉的**全表扫描**访问方式

11. fulltext：

    使用**全文索引**的访问方式

小结：

![image-20220303191907344](image-20220303191907344.png)

#### 4.6.3.6 possible_keys 与 key

在 EXPLAIN 语句输出计划中，possible_keys 表示查询语句中，对某个表执行**单表查询时可能用到的索引有哪些**(一般 WHERE 后面的字段只要有相关索引都会列举出来)。key 列表示**实际用到的索引**有哪些，如果为 NULL，则表示没有使用索引。

**key 并非是 possible_keys 的子集**，有可能出现字段没有相关索引，possible_keys 字段为 NULL，但是经过查询引擎优化，使用了某个其他不相关的索引，key 字段不为空。

#### 4.6.3.7 key_len

key_len 表示实际用到的索引长度（即字节数），帮你检查**是否充分的利用上了索引**，**这个值越大越好**。

主要针对于联合索引，有一定的参考意义。因为联合索引的长度有时候不会全部用上，这里展示的就是用上了多少的长度。越长表示联合索引的利用率就越高。

#### 4.6.3.8 ref

当使用索引列等值查询时，与索引列进行等值匹配的**对象信息**。这个字段记录的是对象信息。

```mysql
select * from s1 where key1 = 'a'; #ref : const
select * from s1 inner join s2 on s1.id = s2.id; #ref : atguigudb1.s1.id
select * from s1 inner join s2 on s2.key1 = upper(s1.key1); #ref : func
```

#### 4.6.3.9 rows

预估需要读取的记录条数，**这个值越小越好**。因为值越小，对应的记录更有可能在同一个页面中，IO 次数就会越少。

##### 4.6.3.10 filtered

某个表经过搜索条件过滤后剩余记录条数的百分比。如果使用的是索引执行的单表扫描，那么计算时需要估计出满足除使用到对应索引的搜索条件外的其他搜索条件的记录有多少条。

对于单表查询来说，这个值没有太大意义。我们更关注**在连接查询中驱动表对应的执行计划记录的 filtered 值**。

```mysql
select * from s1 where key1 > 'z' and common_field = 'a';
```

![image-20220318102651195](image-20220318102651195.png)

```mysql
select * from s1 inner join s2 on s1.key1 = s2.key1 where s1.common_field = 'a';
```

![image-20220318103124606](image-20220318103124606.png)

![image-20220318103141623](image-20220318103141623.png)

##### 4.6.3.11 Extra（重要）

Extra 列是用来说明一些额外信息，包含不适合其他列中显示但是十分重要的额外信息。我们可以通过这些额外信息来**更准确理解 MySQL 到底如何执行给定的查询语句**。常见的额外信息有以下几个：

1. `No table used` ：当查询语句没有 `FROM` 字句时会提示该额外信息。

2. `Impossible WHERE` ：不可能的过滤条件，当查询语句 `WHERE` 永远为 FALSE 时提示该额外信息。

3. `Using where` ：当我们使用全表扫描来执行对某个表的查询，并且该语句的 `WHERE` 字句中有针对该表的搜索条件时；当使用索引访问来执行对某个表的查询，并且该语句的 `WHERE` 子句中有除了该索引包含的列之外的其他搜索条件时，在‘Extra’列中也会显示 Using Where。

4. `No matching min/max row` ：当查询列有 `MIN、MAX` 聚合函数，但是没有符合 `WEHRE` 字句中的搜索条件的记录时。

   ```mysql
   select min(key1) from s1 where key1 = '不存在的值';
   ```

5. `Using index` ：当我们查询列表包含索引以及搜索条件**只包含属于某个索引的列**（如果是查询 `SELECT *` 那不能使用覆盖索引），也就是在可以使用覆盖索引的情况下，可以避免回表操作（即违背了最左规则，使用覆盖索引进行查询）。可以使用到索引的情况就是这个字段。

6. `Using index condition` ：有些搜索条件虽然出现了索引列，但是却不能使用到索引。有时候产生回表操作，我们可以使用一种改进方式剩下回表操作，这种改进称之为**索引条件下推**(见下方例子)。如果查询语句的执行过程中使用索引条件下推这个特性，就会显示 `Using index condition`

   ```SQL
   SELECT * FROM s1 WHERE key1 > 'z' AND key1 LIKE '%a';
   ```

   ![image-20220303204353541](image-20220303204353541.png)

7. `Using join buffer (Block Nested Loop)` ：在连接查询执行过程中，当被驱动表不能有效的利用索引加快访问速度，MySQL 一般会为其分配一块名叫 `join buffer` 的内存块来加快查询速度，也就是我们所讲的**基于块的嵌套循环算法**。

8. `Not exists` ：当我们使用左外连接时，如果 `WHERE` 字句中包含要求被驱动表的某个列等于 NULL 值的搜索条件，而那个列又是不允许存储 NULL 值的，那么改变的执行计划会显示 `Not exists`

   ```sql
   EXPLAIN SELECT * FROM s1 LEFT JOIN s2 ON s1.key1 = s2.key1 WHERE s2.id IS NULL;
   # 设定的表中S2的id字段不可能为NULL
   ```

9. `Using intersect(...) 、 Using union(...) 和 Using sort_union(...)` ：准备使用 `Intersect、Union、Sort-Union` 索引合并的方式执行查询

   ```sql
    EXPLAIN SELECT * FROM s1 WHERE key1 = 'a' OR key3 = 'a';
   ```

10. `Zero limit` ：当 `LIMIT` 字句的参数为 0 时，提示该信息

11. `Using filesort` ：如果某个查询需要使用文件排序的方式执行查询（如果使用索引排序的方式查询，则是 NULL）

    ```sql
    # 按照key1进行排序，但是key1有索引，Extra为NULL
    EXPLAIN SELECT * FROM s1 ORDER BY key1 LIMIT 10;
    # 按照common_field进行排序，common_field没有索引，使用文件排序，Extra为Using filesort
    EXPLAIN SELECT * FROM s1 ORDER BY common_field LIMIT 10;
    ```

    ![image-20220303205355786](image-20220303205355786.png)

12. `Using temporary` ：在许多查询的执行过程中，MySQL 可能会借助临时表来完成一些功能，比如去重、排序等。如果查询中是用到了内部的临时表，在执行计划的 `Extra` 列将会显示 `Using temporary`

    ```sql
    EXPLAIN SELECT DISTINCT common_field FROM s1;
    ```

    在许多包含 `DISTINCE、GROUP BY、UNION` 等字句的查询过程中，**如果不能有效利用索引来完成，MySQL 很有可能会通过建立内部临时表来执行查询**。

小结：

- `EXPLAIN` 不考虑各种 cache
- `EXPLAIN` 不能显示 MySQL 在执行查询时所作的优化工作
- `EXPLAIN` 不会告诉你关于触发器、存储过程的信息或者用户自定义函数对查询的影响情况
- `EXPLAIN` 的部分统计信息是估算的，并非精确值。

## 4.7 `EXPLAIN` 的进一步使用

### 4.7.1 `EXPLAIN` 的四种输出格式

`EXPLAIN` 有四种输出格式：传统格式、JSON 格式、Tree 格式以及可视化输出。

![image-20220303210350619](image-20220303210350619.png)

![image-20220303210424235](image-20220303210424235.png)

![image-20220318152145353](image-20220318152145353.png)

![image-20220303210448130](image-20220303210448130.png)

### 4.7.2 `SHOW WARNINGS` 的使用

![image-20220303211327382](image-20220303211327382.png)

但是要注意，Message 字段展示的信息类似于查询优化器将我们的查询语句重写后的语句，但它并不是标准的查询语句，在很多情况下不能直接运行，它只是作为一个理解 MySQL 如何执行查询语句的一个参考依据而已。

## 4.8 分析优化器执行计划：trace

`OPTIMIZER_TRACE` 是 MySQL5.6 引入的一项跟踪功能，它可以跟踪优化器做出的各种决策（比如访问表的方法、各种开销计算、各种转换等），并将跟踪结果记录到 `INFORMATION_SCHEMA.OPTIMIZER_TRACE` 表中。

此功能默认关闭。开启 trace，并设置格式为 JSON，同时设置 trace 最大能够使用的内存大小，避免解析过程中因为默认内存过小而不能够完整展示。

```mysql
SET optimizer_trace='enabled=on', end_markers_in_json=on;

SET optimizer_trace_max_mem_size=1000000;
```

开启后，可分析如下语句：

- select
- insert
- replace
- update
- delete
- explain
- set
- declare
- case
- if
- return
- call

测试：执行如下 SQL 语句

```mysql
select * from student where id < 10;
```

最后，查询 information_schema. optimizer_trace 就可以知道 MySQL 是如何执行 SQL 的 ：

```mysql
select * from information_schema.optimizer_trace\G
```

![image-20220318153009765](image-20220318153009765.png)

![image-20220318153040569](image-20220318153040569.png)

![image-20220318153106287](image-20220318153106287.png)

... ... ... ... ... ... 具体看 PDF

## 4.9 MySQL 监控分析视图-sys schema

具体看 PDF

# 索引优化与查询优化

都有哪些维度可以进行数据库调优：

- 索引失效、没有充分利用到索引——索引建立
- 关联查询太多 JOIN（设计缺陷或不得已的需求）——SQL 优化
- 服务器调优及各个参数设置（缓冲、线程数等）——调整 my. cnf
- 数据过多——分库分表

虽然 SQL 查询优化的技术有很多，但是大方向上可以分成**物理查询优化**和**逻辑查询优化**两大块。

- 物理查询优化就是通过**索引和表连接方式**等技术来进行优化，这里重点需要掌握索引的使用。
- 逻辑查询优化就是通过**SQL 等价变化**提升查询效率，直白一点就是说，换一种查询写法执行效率可能更高。

## 5.1 索引失效案例

MySQL 中提高性能的一个有效方式就是设计合理的索引。

- 使用索引可以快速定位表中的某条记录，从而提高数据库查询的速度。
- 如果查询时没有使用索引，查询语句就会扫描表中的所有记录。在数据量很大的时候会很缓慢。

其实，用不用索引，最终都是优化器说了算。SQL 语句是否使用索引和**数据库版本，数据量，数据选择度**都有关系。

### 5.1.1 全值匹配我最爱

当 `WHERE` 条件中出现了很多列的时候，我们针对这些列创建的**联合索引**会比只针对某一个列创建的索引要好很多（省去了回表的重复查询操作），这里的全值匹配就是说，**联合索引和 `WHERE` 条件全值匹配是最优解**。

### 5.1.2 最佳左前缀规则

```sql
# 先创建三个索引
CREATE INDEX idx_age ON student(age);
CREATE INDEX idx_age_classid ON student(age,classId);
CREATE INDEX idx_age_classid_name ON student(age,classId,name);

# 使用到了idx_age索引，没有使用idx_age_classid_name索引，因为name是最后一个字段，没有classId就无法匹配name，所以使用idx_age_classid_name索引和idx_age索引是一样的。如果删除了idx_age索引,那么这里就会使用idx_age_classid_name索引
EXPLAIN SELECT * FROM student WHERE student.age = 30 AND student.name = 'abcd';
# 没有使用到索引，因为索引创建时按照最左原则，必须先匹配age字段，这里where没有age，并且是全部查询，不会使用索引
EXPLAIN SELECT * FROM student WHERE classid = 1 AND student.name = 'abcd';
# 具有age字段，使用联合索引
EXPLAIN SELECT * FROM student WHERE classid = 4 AND student.age = 30 AND student.name = 'abcd';
```

结论：MySQL 可以为多个字段创建索引，一个索引可以包括 16 个字段。对于多列索引，**过滤条件要使用索引必须按照索引建立时的顺序，一次满足，一旦跳过某个字段，索引后面的字段都无法被使用**。如果查询条件中没有使用这些字段的第一个字段时，多列索引不会被使用（**除非发生索引覆盖情况**）

### 5.1.3 主键插入顺序

对于 InnoDB 存储引擎的表来说，没有显示创建索引时，表中的数据实际都是存储在聚簇索引的叶子结点的。数据页和记录又是按照记录主键值从大到小的顺序进行排序，如果我们插入的记录**主键值忽大忽小**会比较麻烦。

当一页的数据已经满了以后，再向这个页中插入一条数据（主键值忽大忽小造成），我们需要把当前**页面分裂**成两个页面，把本页中的一些记录移动到新创建的这个页中。页面分裂和记录移位意味着会增加性能消耗。所以我们建议最好让插入的记录的**主键值依次递增**。

### 5.1.4 计算、函数、类型转换（自动或手动）导致索引失效

```sql
CREATE INDEX idx_name ON student(NAME);
# 此语句比下一条语句要好，因为这个语句可以使用索引
EXPLAIN SELECT SQL_NO_CACHE * FROM student WHERE student.name LIKE 'abc%';
# 使用某些函数后，我们无法知道得到的结果，就需要进行全表扫描将每个字段取出来进行函数运算后再进行等值判断
EXPLAIN SELECT SQL_NO_CACHE * FROM student WHERE LEFT(student.name,3) = 'abc';

CREATE INDEX idx_sno ON student(stuno);
# 此语句比下一条语句要好，因为这个语句可以使用索引
EXPLAIN SELECT SQL_NO_CACHE id, stuno, NAME FROM student WHERE stuno = 900000;
# 做运算之后就无法使用索引，需要全表扫描将每个字段取出来来进行操作后再等值判断
EXPLAIN SELECT SQL_NO_CACHE id, stuno, NAME FROM student WHERE stuno+1 = 900001;
```

### 5.1.5 类型转换导致索引失效

```sql
# 假设name字段上设置有索引
# 未使用到索引
EXPLAIN SELECT SQL_NO_CACHE * FROM student WHERE name = 123;
# 使用到索引
EXPLAIN SELECT SQL_NO_CACHE * FROM student WHERE name = '123';
# name=123发生类型转换，索引失效。
```

### 5.1.6 范围条件右边的列索引失效

```sql
CREATE INDEX idx_age_classid_name ON student(age,classId,name);
# 这个语句使用联合索引，但是name字段无法使用联合索引，因为classId是范围条件
WHERE student.age=30 AND student.classId>20 AND student.name = 'abc' ;

# 解决方式：改变联合索引的顺序，将范围查询条件放置语句最后
create index idx_age_name_classid on student(age,name,classid);
EXPLAIN SELECT SQL_NO_CACHE * FROM student WHERE student.age=30 AND student.name = 'abc' AND student.classId>20;
```

**创建联合索引中，务必把范围涉及到的字段写到最后。**

### 5.1.7 不等于（`!=或<>`）索引失效

```sql
CREATE INDEX idx_name ON student(NAME);
# 不等于无法使用索引
EXPLAIN SELECT * FROM student WHERE student.name != 'abc';
```

### 5.1.8 is null 可以使用索引，is not null 无法使用索引

```sql
EXPLAIN SELECT SQL_NO_CACHE * FROM student WHERE age IS NULL;
# is not null需要挨个判断，进行全表扫描，无法使用索引
EXPLAIN SELECT SQL_NO_CACHE * FROM student WHERE age IS NOT NULL;
```

最好在设计数据表时，就将字段设置为`NOT NULL`约束。同理，在查询中使用`not like`也无法使用索引，导致全表扫描。

### 5.1.9 like 以通配符%开头索引失效

```sql
# 可以使用索引
EXPLAIN SELECT * FROM student WHERE student.name != 'ab%';
# 不能使用索引
EXPLAIN SELECT * FROM student WHERE student.name != '%ab';

# 一上来直接使用通配符，无法知道到底要匹配哪个字段（想象一下B+树匹配规则），只能全表扫描。开头确定的情况下，可以进行索引查找
```

页面搜索严禁左模糊或者全模糊，如果需要请走搜索引擎来解决。

#### 5.1.10 `OR` 前后存在非索引的列，索引失效

```sql
CREATE INDEX idx_age ON student(age);
# 未使用到索引,这里classid没有对应索引
EXPLAIN SELECT SQL_NO_CACHE * FROM student WHERE age = 10 OR classid = 100;

CREATE INDEX idx_name ON student(NAME);
#使用到索引,这里的EXPLAIN输出的type字段为index_merge，SQL语句被查询优化器优化成了union，进行了索引合并，两个字段都走了索引
EXPLAIN SELECT SQL_NO_CACHE * FROM student WHERE age = 10 OR name = 'Abel';
```

#### 5.1.11  数据库和表的字符集统一使用 utf8mb4

统一使用 utf8mb4 ( 5.5.3 版本以上支持) 兼容性更好，统一字符集可以避免由于字符集转换产生的乱码。**不同的字符集进行比较前需要进行转换会造成索引失效**。

一般性建议：

- 对于单列索引，尽量选择针对当前 query 过滤性更好的索引
- 在选择联合索引时，当前 query 中过滤性最好的字段在索引字段顺序中，越靠前越好
- 在选择联合索引时，尽量选择能够包含当前 query 中的 where 字句中更多字段的索引
- 在选择联合索引时，如果某个字段可能出现范围查询，尽量把这个字段放在索引次序的最后面

## 5.2 关联查询的优化

### 5.2.1 采用左外连接

![image-20220304105933800](image-20220304105933800.png)

![image-20220304110048794](image-20220304110048794.png)

最后我们删除了被驱动表的索引，可以发现，被驱动表的优化策略就被加入到缓存当中（Extra 字段为 `Using join buffer`）

**在左外连接中，按照 SQL 顺序，左边就是驱动表，右边就是被驱动表。**

### 5.2.2 采用内连接

![image-20220304110505799](image-20220304110505799.png)

![image-20220304110518379](image-20220304110518379.png)

**这里内连接和左外连接不太一样，对于内连接来说，这里左右两个表的地位是一样的。查询优化器可以自己选择哪个做驱动表，哪个做被驱动表。和 SQL 顺序无关。**

如果两个表中的记录数量不一样，并且在两个表的连接条件都存在索引的情况下，**查询优化器会选择小表作为驱动表**。

![image-20220304110535200](image-20220304110535200.png)

**在内连接中，如果只能有一个表的字段有索引，那查询优化器会自动将带有索引字段的表作为被驱动表。**

### 5.2.3 `JOIN` 语句原理

MySQL5.5 之前，MySQL 只支持一种表间关联方式，就是**嵌套循环**。如果关联表的数据量很大，则 join 关联的执行时间会非常长。在 MySQL5.5 之后的版本，MySQL 通过**引入 BNLJ 算法来优化嵌套执行**。

#### 5.2.3.1 驱动表和被驱动表

驱动表就是主表，被驱动表就是从表、非驱动表。

![image-20220318164727123](image-20220318164727123.png)

**无论是内连接还是外连接，查询优化器都有可能会优化我们的 SQL 语句顺序，改变驱动表和非驱动表！**

#### 5.2.3.2 简单嵌套循环连接 (SNLJ)

![image-20220304112315220](image-20220304112315220.png)

![image-20220304112400986](image-20220304112400986.png)

#### 5.2.3.3 索引嵌套循环连接 (INLJ)

![image-20220304112643165](image-20220304112643165.png)

![image-20220304112657600](image-20220304112657600.png)

#### 5.2.3.4 块嵌套循环连接 (BNLJ)

![image-20220304113041776](image-20220304113041776.png)

![image-20220304113102595](image-20220304113102595.png)

![image-20220304113320919](image-20220304113320919.png)

![image-20220318170318107](image-20220318170318107.png)

#### 5.2.3.5 `JOIN` 小结

1. INLJ > BNLJ > SNLJ

2. 永远用小**结果集**驱动大**结果集**（其本质就是减少外层循环的数据数量）（小的度量单位指的是**表行数 * 每行大小**）

   ![image-20220318170753216](image-20220318170753216.png)

3. 为被驱动表匹配的条件增加索引（减少内存表的循环匹配次数）

4. 增大 join buffer size 的大小（一次缓存的数据越多，那么内层包的扫描次数就越少）

5. 减少驱动表不必要的字段查询（字段越少，join buffer 所缓存的数据就越多）

#### 5.2.3.6 Hash join

![image-20220304193138725](image-20220304193138725.png)

![image-20220304193248141](image-20220304193248141.png)

### 5.2.4 小结

![image-20220320185749262](image-20220320185749262.png)

## 5.3 子查询优化

子查询可以一次性完成很多逻辑上需要多个步骤才能完成的 SQL 操作。

但是，子查询的执行效率不高：

- 执行子查询，MySQL 需要为内层查询语句产生的结果**建立一个临时表**，然后外层查询语句从临时表中查询记录，最后撤销这些临时表。这个过程会消耗过多的 CPU 和 IO 资源，产生大量慢查询。
- 子查询的结果集存储的临时表，不论是内存临时表还是磁盘临时表都 `不会存在索引`，查询性能低。
- 对于返回结果集比较大的子查询，其对查询性能的影响就越大。

**注意：我们一般开发中为了性能优化，不建议使用子查询，建议将子查询 SQL 拆开结合程序多次查询，或者使用 `JOIN` 字句来代替**，连接查询不需要建立临时表，其速度比子查询要快，如果查询中使用索引的话，性能就会更好。

```mysql
#子查询的优化
#创建班级表中班长的索引
create index idx_monitor on class(monitor);

#查询班长的信息-子查询
explain select * from student stu1
where stu1.`stuno` in(
    select monitor from class c
    where monitor is not null
);
#查询班长的信息-join
explain select stu1.* from student stu1 join class c
on stu1.`stuno` = c.`monitor`
where c.`monitor` is not null;

#查询不为班长的学生信息-子查询
explain select * from student stu1
where stu1.`stuno` not in(
    select monitor from class c
    where monitor is not null
);
#查询不为班长的学生信息-join
explain select stu1.* from student stu1 left outer join class c
on stu1.`stuno` = c.`monitor`
where c.`monitor` is null;
```

尽量用 `LEFT JOIN xxx ON WHERE xx IS NULL` 来代替 `NOT IN` 或者 `NOT EXISTS`

## 5.4 排序优化

### 5.4.1 排序优化重点

在 MySQL 中，支持两种排序方式 `FileSort` 和 `Index`

- `Index` 排序中，索引可以保证数据的有序性，效率很高
- `FileSort` 排序则一般在**内存中**进行排序，占用 CPU 较多，效率很低。

优化建议：

1. SQL 中可以在 `WHERE` 和 `ORDER BY` 字句中使用索引。前者是为了**避免全表扫描**，后者是为了**避免使用 `FileSort` 排序**
2. 尽量使用 Index 完成 `ORDER BY` 排序。如果 `WHERE` 和 `ORDER BY` 后面是相同的列就用单索引列；如果不同就使用联合索引
3. 无法使用 Index 时，需要对 `FileSort` 排序进行调优

`ORDER BY` 索引失效的情况：

1. `SELECT` 后面字段太多，走非聚簇索引时会造成回表操作（需要先查到对应的主键，然后根据主键回表再查到需要的字段），还不如不走索引直接全表扫描查找。

2. 不使用 `LIMIT` 字段，会导致查询出来太多记录，走非聚簇索引会产生回表操作，如果使用 `LIMIT`，因为记录很少，即使发生回表，也比全表扫描快。

3. `ORDER BY` 时顺序错误，导致索引失效。比如先 `ORDER BY` 联合索引的第二个字段，再按照第一个字段，就无法使用索引。使用联合索引要遵守最左原则。

4. `ORDER BY` 时规则不一致，索引失效。多个字段在排序中反复升序降序，就不会使用索引。即排序多个字段要么都是升序，要么都是降序。否则索引会失效。<!--但是所有字段都与索引字段反序就可以走索引，倒着扫描-->

5. 无过滤，不索引。有 `WHERE` 条件之后，可以使用索引先过滤，如果过滤之后的数据量很小，查询优化器可能就不会使用索引再查找排序了。如果再加上 `LIMIT` 之后，那么大概率会使用索引。

   ```mysql
   # index student(age,classid,stuno)
   select * from student where age=45 order by classid; #【key_len:5】age=45走索引
   select * from student where classid=45 order by age; #不扫描辅助索引，很多时候都是优化器决定的，它觉得不值得用【尝试：只select name 时会走索引吗？】
   select * from student where classid=45 order by age limit 10; #【type:index】扫描辅助索引，个人认为这里直接扫描辅助索引的叶子节点，直到找到10个符合classid=45的记录。
   ```

结论：

1. 两个索引同时存在时，MySQL 查询优化器会自动选择最优的方案，但是**随着数据量的变化，选择的索引也会随之变化**。
2. 当**范围条件（`WHERE` 字句）**和 `group by` 或者 `order by` 的字段出现二选一时，优先观察**条件字段（`WHERE` 字句）过滤的数量**，如果过滤的数据足够多，而排序的数据并不多时，优先把索引放在范围条件字段上，反之亦然。

### 5.4.2 filesort 算法：双路排序和单路排序

![image-20220304204255063](image-20220304204255063.png)

![image-20220318193244287](image-20220318193244287.png)

### 5.5-0 `GROUP BY` 优化

![image-20220304204701494](image-20220304204701494.png)

### 5.5-1 优化分页查询

![image-20220318193658287](image-20220318193658287.png)

![image-20220318194759550](image-20220318194759550.png)

## 5.6 覆盖索引

### 5.6.1 什么是覆盖索引？

![image-20220304210010849](image-20220304210010849.png)

**一个索引包含了满足查询结果的数据就叫做覆盖索引**。简单来说就是，**索引列 + 主键**包含 `SELECT` 到 `FROM` 之间查询的列。

### 5.6.2 覆盖索引的利弊

好处：

1. 避免 Innodb 表进行行索引的二次查询（回表）
2. 可以把**随机 IO**变成**顺序 IO**，加快查询效率（随机 IO 是因为回表的时候可能会产生）

由于覆盖索引可以减少树的搜索次数，显著提升查询性能。所以使用覆盖索引是一个常用的性能优化。

弊端：索引字段的维护总是要付出代价的。因此，在建立冗余索引来支持索引覆盖就需要权衡考虑。（比如单纯为了这个 `SELECT` 查询的内容，而故意添加了更多列的联合索引，为了让 SQL 走覆盖索引而增加了索引本身的开销，需要权衡利弊）

## 5.7 如何给字符串添加索引

### 5.7.1 前缀索引

![image-20220305152844952](image-20220305152844952.png)

![image-20220305152904656](image-20220305152904656.png)

![image-20220305152921995](image-20220305152921995.png)

总结：前缀索引的索引结构**占用的空间会更小**，这是前缀索引的优势。但是同时带来的损失是，**可能会增加额外的记录扫描次数**。

### 5.7.2 前缀索引对覆盖索引的影响

使用前缀索引就用不上覆盖索引对查询性能的优化了，这也是你在选择是否使用前缀索引时需要考虑的一个因素。因为联合索引中的某个字段如果采用前缀索引，则覆盖索引的功能就**无法准确识别**（**添加了前缀索引本身就让查找变得模糊了**），所以不能通过避免回表的方式准确定位直接读取某行值。（假设不用前缀索引，联合索引只会找到一行记录，然后通过覆盖索引就可以避免回表操作；但是如果使用前缀索引，那联合索引就会找到 n 条记录，即使使用覆盖索引也不能进一步确定到底是哪一条记录，必须进行进一步的回表操作）

## 5.8 索引条件下推（ICP）

ICP 是用来减少回表操作所需要的消耗，当 `WHERE` 条件中超过一个规则在同一个非聚簇索引中，就可以使用 ICP。

默认的情况下，SQL 需要根据第一个 `WHERE` 字句的条件查询后进行回表操作（非聚簇索引），然后根据查询出来这些满足第一个 `WHERE` 条件的行，进行之后的 `WHERE` 条件过滤。

使用 ICP 后，SQL 会先根据索引里面相关的内容，在第一次使用非聚簇索引查询的时候，直接过滤掉大量的不需要的列，保证回表操作需要查找数目不会很多，优化 SQL 的查询时间。

![image-20220305094848984](image-20220305094848984.png)

索引条件下推很多情况下是针对联合索引。

简单来说，如果**多个过滤条件**在**同一个非聚簇索引**中存在，就会发生索引条件下推。

ICP 的使用条件：

- 如果表访问的类型为：`range、ref、eq_ref、ref_or_null` 可以使用 ICP
- ICP 可以用于 InnoDB 和 MyISAM 表，包括分区表 InnoDB 和 MyISAM 表
- 对于 InnoDB 表，ICP 仅用于**二级索引**。ICP 的目标是减少全行读取的次数，从而减少 IO 操作
- 当使用覆盖索引时，不支持 ICP（因为覆盖索引就不会回表，而 ICP 是为了减少回表的消耗）
- **相关子查询条件不能使用 ICP**

## 5.9 其他查询优化策略

### 5.9.1 `EXISTS` 和 `IN` 的区别

两者的区别其实就是驱动表的区别，我们将选择的标准规定为：**小表驱动大表**。这种方式下，查询的效率是最高的。

![image-20220305101247881](image-20220305101247881.png)

### 5.9.2 `COUNT(*)` 与 `COUNT(具体字段)` 效率

MySQL 中统计表的行数，可以使用三种方式：`SELECT COUNT(*) 、SELECT COUNT(1) 、SELECT COUNT(具体字段)`，这三者的查询效率是怎样的？

前提：如果你要统计的是某个字段的非空数据行数，则另当别论，毕竟比较执行效率的前提是结果一样才可以。

环节 1：`COUNT(*)和COUNT(1)` 都是对所有结果进行 COUNT，二者在本质上没有区别。如果有 `WHERE` 字句，则是对所有符合筛选条件的数据行进行统计；如果没有 `WHERE` 字句，则是对数据表的数据行数进行统计。

环节 2：如果是 MyISAM 存储引擎，统计数据表行数只需要 O (1) 时间复杂度，因为每张表都保存了一个 row_count 值；如果是 InnoDB 存储引擎，支持事务，采用行级锁和 MVCC 机制，因此需要扫描全表，是 O (n) 时间复杂度。

环节 3：在 InnoDB 中，如果采用 `COUNT(具体字段)` 来统计行数，**尽量采用二级索引**。因为主键采用的是聚簇索引，聚簇索引包含的信息多，明显会大于二级索引。对于 `COUNT(*)和COUNT(1)` 来说，他们不需要查找具体的行，系统会自动采用占用空间更小的二级索引来进行统计。如果有多个二级索引，会使用 key_len 小的二级索引进行扫描（`EXPLAIN` 的字段）

### 5.9.3 `SELECT(*)`

表查询中，尽量使用明确字段：

- MySQL 在解析过程中，会通过 `查询数据字典` 将 `*` 转换成所有列名，需要耗费时间。
- 无法使用覆盖索引

### 5.9.4 `LIMIT 1` 对优化的影响

如果是**全表扫描**，如果确定结果集只有一条，加上 `LIMIT 1` 会加快查询速度。

如果数据表已经对字段建立了**唯一索引**，那么可以通过索引进行查询，就不需要使用 `LIMIT 1` 了，因为本身的索引找到这一条记录就已经结束了，`LIMIT 1` 不会优化很多，最多是常数级别的优化（使用索引的话，在数据页中找到对应行数据后，往后再找一个也就不满足条件了，根据索引 B+树的规则就会直接退出）

### 5.9.5 多使用 `COMMIT`

只要有可能，在程序中尽量使用 `COMMIT`，这样程序性能可以得到提高。

`COMMIT` 释放的资源：

- 回滚段上用于恢复数据的信息
- 被程序语句获得的锁
- redo / undo log buffer 中的空间
- 管理上述三种资源的内部花费

### 5.10 淘宝数据库的主键是如何设计的？

![image-20220305103959442](image-20220305103959442.png)

![image-20220305104203179](image-20220305104203179.png)

![image-20220305104604987](image-20220305104604987.png)



![image-20220305104854296](image-20220305104854296.png)

![image-20220305105223512](image-20220305105223512.png)

![image-20220305105157366](image-20220305105157366.png)

![image-20220305105708819](image-20220305105708819.png)



# 数据库的设计规范

![image-20220320200327345](image-20220320200327345.png)

良好的数据库设计有以下优点：

- 节省数据的存储空间
- 保证数据的完整性
- 方便进行数据库应用系统的开发

总之，开始设置数据库的时候，我们就需要重视数据表的设计。为了建立冗余较小、结构合理的数据库，设计数据库时必须遵循一定的规则。

## 6.1 范式

在关系型数据库中，关于数据表设计的基本原则、规则就称为范式。范式是关系数据库理论的基础，也是我们设计数据库结构过程中所要遵循的**规则和指导方法**。

目前关系型数据库有六种常见范式，按照范式级别，从低到高分别是：**第一范式（1NF）、第二范式（2NF）、第三范式（3NF）、巴斯-科德范式（BCNF）、第四范式（4NF）和第五范式（5NF，又称为完美范式）**

数据库的范式设计越高阶，冗余度就越低。同时高阶的范式一定满足低阶范式的要求。满足最低要求的范式是第一范式，在第一范式基础上进一步满足更多规范要求的称为第二范式。

一般来说，在关系型数据库设计中，**最高也就遵循到 BCNF（巴斯范式），普遍还是第三范式**。有时候为了提高某些查询性能，我们还需要破坏范式规则，也就是**反范式化**。

![image-20220305155727090](image-20220305155727090.png)

### 6.1.1 键和相关属性的概念

范式的定义会使用到主键和候选键，数据库中的键由一个或者多个属性组成。数据表中常用的几种键和属性的定义：

- 超键：能唯一表示元组的属性及叫做超键。
- 候选键：如果超键不包括多余的属性，那么这个超键就是候选键。
- 主键：用户可以从候选键中选择一个作为主键。
- 外键：如果数据表 R1 中的某属性集不是 R1 的主键，而是另一个数据表 R2 的主键，那这个属性集就是数据表 R1 的外键。
- 主属性：包含在任一候选键中的属性称为主属性。
- 非主属性：与主属性相对，指的是不包含在任何一个候选键中的属性。

通常，我们也将候选键称之为**码**，把主键称为**主码**。

![image-20220305160312398](image-20220305160312398.png)

### 6.1.2 第一范式（1st NF）

第一范式主要是确保数据表中某个字段的值必须具有**原子性**，也就是说数据表中每个字段的值为**不可再次拆分**的最小数据单元。

事实上，任何的 DBMS 都会满足第一范式的要求，不会将字段进行拆分。

![image-20220305160513565](image-20220305160513565.png)

![image-20220305160532975](image-20220305160532975.png)

![image-20220305160542532](image-20220305160542532.png)

![image-20220305160633463](image-20220305160633463.png)

### 6.1.3 第二范式（2nd NF）

第二范式要求，在满足第一范式的基础上，还要**满足数据表里的每一条数据记录，都是可唯一标识的（即必须有主键）。而且所有非主键字段，都必须完全依赖主键，不能只依赖主键的一部分。**如果知道主键的所有属性值，就可以检索到任何元组（任意一行）的任何属性的任何值（要求中的主键，其实可以拓展替换为候选键）

![image-20220305161316739](image-20220305161316739.png)

![image-20220305161332889](image-20220305161332889.png)

![image-20220305162110909](image-20220305162110909.png)

小结：第二范式要求实体的属性完全依赖主关键字。如果存在不完全依赖，那么这个属性和主关键字的这一部分应该分离出来形成一个新的实体，新实体与原实体之间是一对多的关系。

### 6.1.4 第三范式（3rd NF）

第三范式是在第二范式的基础上，确保数据表中的每一个非主键字段都和主键字段**直接相关**，也就是说，**要求数据表中的所有非主键字段不能依赖于其他非主键字段。**（即，不能存在非主属性 A 依赖于非主属性 B，非主属性 B 依赖于主键 C 的情况，即存在 A->B->C 的决定关系）通俗的讲，该规则的意思就是所有**非主键属性**之间不能有依赖关系，必须相互独立。（这里的主键可以拓展为候选键）

![image-20220305162804667](image-20220305162804667.png)

![image-20220305162908563](image-20220305162908563.png)

![image-20220305163016784](image-20220305163016784.png)

![image-20220305163136050](image-20220305163136050.png)

2NF 和 3NF 通常以这句话来概括：**每个非主键属性依赖于主键，依赖于整个主键，并且除了依赖于主键不依赖于任何其他键。**

### 6.1.5 小结

对于数据表的设计，有三个范式要遵循。

1. 第一范式（1NF），确保每列保持**原子性**
2. 第二范式（2NF），确保每列都和主键**完全依赖**
3. 第三范式（3NF），确保每列都和主键列**直接相关**，而不是间接相关。

范式的优点：数据标准化有助于消除数据库中的**数据冗余**。第三范式被认为在性能、扩展性和数据完整性方面达到了平衡。

范式的缺点：范式的使用，可能会**降低查询的效率**，进行查询的时候可能需要**关联多张表**，可能会使一些**索引策略无效**。

范式只是提出了设计的标准，实际上设计数据表时，未必一定符合这些标准。开发中，我们可以通过**增加少量的冗余**来提高数据库的**读性能**，减少关联查询，join 表的次数，实现**空间换取时间的目的**。

## 6.2 反范式化

如果数据库中的数据量比较大，系统的 UV 和 PV 访问频次很高，如果按照三大范式设计，读数据会产生大量的关联查询。如果我们想对查询效率进行优化，**反范式化**也是一种思路。此时，我们可以通过在数据表中**增加冗余字段**来提高数据库的读性能。

![image-20220305164235886](image-20220305164235886.png)

![image-20220305164629269](image-20220305164629269.png)

![image-20220305164835673](image-20220305164835673.png)

反范式可以通过空间换取时间，提升查询的效率，但是反范式化会带来一些新问题：

1. 存储空间变大了
2. 一个表中字段做了修改，另一个表中冗余的字段也需要做同步修改，否则产生数据不一致的问题
3. 若采用存储过程来支持数据的更新、删除等额外操作，如果更新频繁，会非常消耗系统资源
4. 在数据量小的情况下，反范式不能体现出性能的优势，可能还会让数据库的设计变得更加复杂

反范式的适用场景:

当冗余信息有价值或者能**大幅度提高查询效率**的时候，我们才会采取反范式的优化。

1. 增加冗余字段的建议：增加冗余字段一定要符合如下两个条件。只有满足这两个条件，才可以考虑增加冗余字段。
   - 这个冗余字段**不需要经常修改**
   - 这个冗余字段**查询的时候不可或缺**。
   
2. 历史快照、历史数据的需要：现实生活中我们经常需要一些冗余信息，比如订单中的收货人信息，包括姓名、电话和地址等。每次发生的订单收获信息都属于历史快照，需要进行保存，但用户可以随时修改自己的信息，这时保存这些冗余信息是非常有必要的。

   反范式优化也常用在**数据仓库**的设计中，因为数据仓库通常**存储历史数据**，对增删改的实时性要求不强，对历史数据的分析需求强，这时适当允许数据的冗余度，方便进行数据分析。

数据库与数据仓库的区别：

1. 数据库设计的目的在于**捕获数据**，数据仓库的设计目的在于**分析数据**。
2. 数据库对数据的**增删改实时性**强，需要存储在线的用户数据，而数据仓库存储的一般是历史数据。
3. 数据库设计需要**尽量避免冗余**，但为了提高查询效率也允许一定的冗余度。而数据仓库在设计上更偏向采用反范式设计。

## 6.3 BNCF（巴斯范式）

若一个关系达到了第三范式，并且它**只有一个候选键**，或者它的**每个候选键都是单属性**，则该关系自然达到 BC 范式

一般来说，一个数据库设计符合 3NF 或者 BCNF 就可以了。

![image-20220305195055065](image-20220305195055065.png)

![image-20220305195109797](image-20220305195109797.png)

![image-20220305195554468](image-20220305195554468.png)

## 6.4 第四范式

多值依赖的概念：

- 多值依赖即属性之间的一对多关系
- 函数依赖实际上是单值依赖，不能表达属性值之间一对多关系
- 平凡的多值依赖：全集 U = K + A，一个 K 可以对应多个 A。此时表就是一组一对多关系
- 非平凡的多值依赖：全集 U = K + A + B，一个 K 可以对应于多个 A，也可以对应于多个 B，A 与 B 互相独立。整个表有多组一对多关系，且有：“一”部分是相同的属性集合，“多”部分是相互独立的属性集合。

第四范式即在满足巴斯范式的基础上，消除非平凡且非函数依赖的多值依赖（即把同一表内的**多对多关系删除**）

![image-20220305200307708](image-20220305200307708.png)

![image-20220305200317118](image-20220305200317118.png)

## 6.5 第五范式、域键范式

第五范式也叫完美范式。

如果关系模式 R 中的每一个连接依赖均由 R 的候选键所隐含，满足此模式符合第五范式。

第五范式处理的是**无损连接问题，没有实际意义**。

而域键范式试图定义一个**终极范式**，只存在理论研究中。

## 6.6 ER 模型

ER 模型也叫做**实体关系模型**，是用来描述现实生活中客观存在的事物、事物的属性，以及事物之间关系的一种数据模型。**在开发基于数据库的信息系统的设计阶段，通常使用 ER 模型来描述信息需求和信息特性，帮助我们理清业务逻辑，从而设计出优秀的数据库。**

ER 模型中有三个要素，分别是实体、属性和关系：

- 实体可以看做是数据对象，ER 模型中用矩形表示。实体分为两类，分别是强实体和弱实体。强实体是指不依赖于其他实体的实体，弱实体是指对另一个实体有很强的依赖关系的实体。
- 属性是实体的特征，比如超市地址、联系电源、员工数等。在 ER 模型中用椭圆形来表示。
- 关系指的是实体之间的联系，比如超市把物品卖给顾客，就是一种关系。在 ER 图中用菱形表示。

实体和属性不容易区分。**可以独立存在的是实体，不可再分的是属性**。也就是说，属性不能包含其他属性。

关系的类型：一对一、一对多、多对多三种关系

![image-20220305202533408](image-20220305202533408.png)

ER 模型转换数据表的规则：

- 一个实体通常转换成一个数据表
- 一个多对多的关系也会转换成一个数据表
- 一个一对一或者一对多的关系，通过表的外键来表达
- 属性转换成表的字段

![image-20220305203854788](image-20220305203854788.png)

## 6.7 数据表的设计原则

综合以上内容，总结数据表的设计原则——“三少一多”

1. 数据表的个数越少越好。证明实体和联系设计得越简洁，既方便理解又方便操作。
2. 数据表中字段个数越少越好。设置字段个数少的前提是各个字段相互独立，而不是某个字段的取值可以由其他字段计算出来。当然字段个数少是相对的，我们通常会在数据冗余和检索效率中进行平衡。
3. 数据表中联合主键的字段个数越少越好
4. 使用主键和外键越多越好（这里的外键不是外键约束，而是外键的关系，在业务逻辑上实现，不是真的添加外键）

“三少一多”的原则核心就是**简单可复用**，希望用更少的表、更少的字段、更少的联合主键字段来完成数据表的设计。可复用则是通过主键和外键的使用来提高表的利用率。

## 6.8 数据库对象的编写建议

![image-20220305204244622](image-20220305204244622.png)

![image-20220305204301117](image-20220305204301117.png)

![image-20220305204320496](image-20220305204320496.png)

![image-20220305204340727](image-20220305204340727.png)



# 数据库其他调优策略

数据库的调优除了 SQL 方式的调优，从这个角度来说，我们思考的维度不仅仅局限在 SQL 优化上了：

1. 选择合适的 DBMS

   ![image-20220306152306326](image-20220306152306326.png)

2. 优化表设计

   ![image-20220306152409736](image-20220306152409736.png)

3. 优化逻辑查询

   ![image-20220306152544432](image-20220306152544432.png)

4. 优化物理查询

   ![image-20220306152743034](image-20220306152743034.png)

5. 使用 Redis 或 Memcacheed 作为缓存

   ![image-20220306153007186](image-20220306153007186.png)

6. 库级优化

   ![image-20220306153128201](image-20220306153128201.png)

![image-20220306153210479](image-20220306153210479.png)

![image-20220306153154781](image-20220306153154781.png)

## 7.1 优化 MySQL 服务器

优化 MySQL 服务器主要从两个方面来优化，一方面是对**硬件**进行优化；另一方面是对 MySQL**服务的参数**进行优化。

### 7.1.1 优化服务器硬件

服务器的硬件性能直接决定 MySQL 数据库的性能：

1. 配置较大的内存，可以通过增加系统的**缓冲区容量**是数据在内存停留时间更惨，减少磁盘 I/O
2. 配置高速磁盘系统，以减少读盘的等待时间，提高响应速度
3. 合理分布磁盘 I/O，把磁盘 I/O 分散在多个设备上，提高并行操作能力
4. 配置多处理器，**MySQL 是多线程的数据库**，多处理器可以同时执行多个线程

### 7.1.2 优化 MySQL 参数

通过优化 MySQL 的参数可以提高资源利用率，配置在 `my.cnf` 或者 `my.ini` 文件的[mysqld]组中，配置完毕后重启生效。

下面是几个性能影响较大的参数：

- `innodb_buffer_pool_size` ：这个参数是 Mysql 数据库最重要的参数之一，表示 InnoDB 类型的**表和索引的最大缓存** 。它不仅仅缓存 **索引数据** ，还会缓存**表的数据** 。这个值越大，查询的速度就会越快。但是这个值太大会影响操作系统的性能。
- `key_buffer_size` ：表示**索引缓冲区的大小** 。索引缓冲区是所有的**线程共享** 。增加索引缓冲区可以得到更好处理的索引（对所有读和多重写）。当然，这个值不是越大越好，它的大小取决于内存的大小。如果这个值太大，就会导致操作系统频繁换页，也会降低系统性能。对于内存在 4GB 左右的服务器该参数可设置为 256M 或 384M 。
- `table_cache` ：表示**同时打开的表的个数** 。这个值越大，能够同时打开的表的个数越多。物理内存越大，设置就越大。默认为 2402
- `query_cache_size` ：表示**查询缓冲区的大小**。可以通过在 MySQL 控制台观察，如果 `Qcache_lowmem_prunes` 的值非常大，则表明经常出现缓冲不够的情况，就要增加 `Query_cache_size` 的值；如果 `Qcache_hits` 的值非常大，则表明查询缓冲使用非常频繁，如果该值较小反而会影响效率，那么可以考虑不用查询缓存；`Qcache_free_blocks`，如果该值非常大，则表明缓冲区中碎片很多。MySQL8.0 之后失效。该参数需要和 `query_cache_type` 配合使用。
- `query_cache_type` 的值是 0 时，所有的查询都不使用查询缓存区。但是 query_cache_type=0 并不会导致 MySQL 释放 `query_cache_size` 所配置的缓存区内存。
  - 当 `query_cache_type=1` 时，所有的查询都将使用查询缓存区，除非在查询语句中指定 `SQL_NO_CACHE` ，如 `SELECT SQL_NO_CACHE * FROM tbl_name`。
  - 当 `query_cache_type=2` 时，只有在查询语句中使用 `SQL_CACHE` 关键字，查询才会使用查询缓存区。使用查询缓存区可以提高查询的速度，这种方式只适用于**修改操作少且经常执行相同**的查询操作的情况。
- `sort_buffer_size` ：表示每个**需要进行排序的线程分配的缓冲区的大小**。增加这个参数的值可以提高 `ORDER BY` 或 `GROUP BY` 操作的速度。
- `join_buffer_size = 8M` ：表示**联合查询操作所能使用的缓冲区大小** ，和 `sort_buffer_size` 一样，该参数对应的分配内存也是每个连接独享。
- `read_buffer_size` ：表示**每个线程连续扫描时为扫描的每个表分配的缓冲区的大小（字节）**。当线程从表中连续读取记录时需要用到这个缓冲区。`SET SESSION read_buffer_size=n` 可以临时设置该参数的值。默认为 64K，可以设置为 4M。
- `innodb_flush_log_at_trx_commit` ：表示何时将缓冲区的数据写入日志文件，并且将日志文件写入磁盘中。该参数对于 innoDB 引擎非常重要。该参数有 3 个值，分别为 0、1 和 2。该参数的默认值为 1。
  - 值为 0 时，表示**每秒 1 次**的频率将数据写入日志文件并将日志文件写入磁盘。每个事务的 commit 并不会触发前面的任何操作。该模式速度最快，但不太安全，mysqld 进程的崩溃会导致上一秒钟所有事务数据的丢失。
  - 值为 1 时，表示**每次提交事务时**将数据写入日志文件并将日志文件写入磁盘进行同步。该模式是最安全的，但也是最慢的一种方式。因为每次事务提交或事务外的指令都需要把日志写入 （flush）硬盘。
  - 值为 2 时，表示**每次提交事务时**将数据写入日志文件， **每隔 1 秒**将日志文件写入磁盘。该模式速度较快，也比 0 安全，只有在操作系统崩溃或者系统断电的情况下，上一秒钟所有事务数据才可能丢失。
- `innodb_log_buffer_size` ：这是**InnoDB 存储引擎的**事务日志所使用的缓冲区。为了提高性能，也是先将信息写入 Innodb Log Buffer 中，当满足 `innodb_flush_log_trx_commit` 参数所设置的相应条件（或者日志缓冲区写满）之后，才会将日志写到文件（或者同步到磁盘）中。
- `max_connections` ：表示允许连接到 MySQL 数据库的最大数量，默认值是 151。如果状态变量 `connection_errors_max_connections` 不为零，并且一直增长，则说明**不断有连接请求因数据库连接数已达到允许最大值而失败**，这是可以考虑增大 `max_connections` 的值。在 Linux 平台下，性能好的服务器，支持 500-1000 个连接不是难事，需要根据服务器性能进行评估设定。这个连接数不是越大越好，因为这些连接会浪费内存的资源。过多的连接可能会导致 MySQL 服务器僵死。
- `back_log` ：用于**控制 MySQL 监听 TCP 端口时设置的积压请求栈大小** 。如果 MySql 的连接数达到 `max_connections` 时，新来的请求将会被存在堆栈中，以等待某一连接释放资源，该堆栈的数量即 `back_log`，如果等待连接的数量超过 `back_log`，将不被授予连接资源，将会报错。5.6.6 版本之前默认值为 50 ，之后的版本默认为 50+（max_connections / 5），对于 Linux 系统推荐设置为小于 512 的整数，但最大不超过 900。如果需要数据库在较短的时间内处理大量连接请求，可以考虑适当增大 `back_log` 的值。
- `thread_cache_size` ：**线程池缓存线程数量的大小**，当客户端断开连接后将当前线程缓存起来，当在接到新的连接请求时快速响应无需创建新的线程。这尤其对那些使用短连接的应用程序来说可以极大的提高创建连接的效率。那么为了提高性能可以增大该参数的值。默认为 60，可以设置为 120。
- `wait_timeout` ：指定**一个请求的最大连接时间**，对于 4GB 左右内存的服务器可以设置为 5-10。
- `interactive_timeout` ：表示服务器在关闭连接前等待行动的秒数。

## 7.2 优化数据库结构

### 7.2.1 拆分表：冷热数据分离

拆分表的思路是，把一个包含很多字段的表拆分成 2 个或者多个相对较小的表。这样做的原因是，可以把某些操作频率很高的字段（热数据）保存下来，而另一些使用频率很低的字段（冷数据）分开保存，实现冷热数据的分离，减小表的宽度。

冷热数据分离的目的：

1. 减少磁盘 IO，保证热数据的内存缓存命中率。
2. 更有效的利用缓存，避免读入无用的冷数据。

### 7.2.2 增加中间表

对于经常联合查询的表，可以建立中间表来提高查询效率。**把经常联合查询的数据变成对中间表的查询**，提高查询效率。

这样处理要注意中间表数据的更新情况，所以不是频繁更新的情况建立中间表比较有效。

### 7.2.3 增加冗余字段

设计数据库表时应该遵循范式理论的规范，尽可能减少冗余字段。但是**合理的加入冗余字段可以提高查询速度**。在 6.2 小节中的反范式化中有具体举例。

### 7.2.4 优化数据类型

优先选择符合存储需要的最小数据类型。因为列的字段越大，建立索引需要的空间也就越大，这样一页中能存储的索引节点的数量也就越少，遍历时需要的 IO 次数也就越多，索引的性能也就越差。

1. 对整数类型进行优化：遇见整数可以使用 `INT` 类型，对于非负性的数据，优先使用 `UNSIGNED` 来存储
2. 既可以使用文本也可以使用整数的字段，使用整数类型：跟文本类型相比较，大整数往往占用更少的存储空间
3. 避免使用 `TEXT、BLOB` 数据类型：MySQL**内存临时表**不支持 `TEXT、BLOB` 这样的大数据类型，如果包含这样数据，必须使用**磁盘临时表**进行。并且对于这种数据，MySQL 还是要进行**二次查询**，性能会变得很差。如果一定要使用，需要把 `TEXT、BLOB`**分离到单独的扩展表中**。
4. 避免使用 `ENUM` 类型，修改 `ENUM` 类型需要使用 `ALTER` 语句。尽量使用 `TINYINT` 代替 `ENUM` 类型
5. 使用 `TIMESTAMP` 存储时间，只需要 4 个字节。
6. 用 `DECIMAL` 代替 `FLOAT` 和 `DOUBLE` 存储精确浮点数。

### 7.2.5 优化插入记录的速度

这里我们分为 MyISAM 引擎和 InnoDB 引擎来讲：

#### 7.2.5.1 MyISAM 引擎的表

1. 禁用索引

   ![image-20220306163102001](image-20220306163102001.png)

2. 禁用唯一性检查

   ![image-20220306163114406](image-20220306163114406.png)

3. 使用批量插入

   ![image-20220306163144849](image-20220306163144849.png)

4. 使用 `LOAD DATA INFILE` 批量导入

#### 7.2.5.2 InnoDB 引擎的表

1. 禁用唯一性检查（和 MyISAM 一样）

2. 禁用外键检查

   ![image-20220306163333825](image-20220306163333825.png)

3. 禁止自动提交

   ![image-20220306163344355](image-20220306163344355.png)

### 7.2.6 使用非空约束

**设计字段的时候，如果业务允许，尽量使用非空约束**。好处如下：

- 进行比较和计算时，省去对 NULL 值字段判断是否为空的开销，提高存储效率
- 非空字段容易创建索引，因为索引的 NULL 列需要额外的空间来保存（见 2.2 页的内部结构），所以要占用更多的空间，使用非空约束，每个字段可以节省 1 个 bit 的存储空间。

### 7.2.7 分析表、检查表与优化表

分析表主要是分析关键字的分布；检查表主要是检查表是否存在错误；优化表主要是消除删除或者更新造成的空间浪费。

![image-20220306163703336](image-20220306163703336.png)

![image-20220306163712798](image-20220306163712798.png)

![image-20220306163728064](image-20220306163728064.png)

## 7.3 大表优化

### 7.3.1 限定查询的范围

**禁止不带任何限制数据范围条件的查询语句。**

### 7.3.2 读写分离

![image-20220306170132921](image-20220306170132921.png)

### 7.3.3 垂直拆分

![image-20220306170258977](image-20220306170258977.png)

![image-20220306170422949](image-20220306170422949.png)

### 7.3.4 水平拆分

![image-20220306170649046](image-20220306170649046.png)

![image-20220306170618672](image-20220306170618672.png)



# 事务的基础知识

## 8.1 数据库事务概述

事务是数据库区别于文件系统的重要特性之一，事务可以保证数据库始终保持**一致性**，同时我们还能通过事务的机制**恢复到某个时间点**，这样可以保证已提交到数据库的修改不会因为系统崩溃而丢失。

在 MySQL 中，只有 InnoDB 是支持事务的。

事务：一组逻辑操作单元，使数据从一种状态变换到另一种状态。

事务处理的原则：保证所有事务都作为**一个工作单元**来执行，即使出现了故障，都不能改变这种执行方式。当在一个事务中执行多个操作时，要么所有的行为都被提交，那么这些修改就永久保存下来；要么数据库管理系统将放弃所作的修改，整个事务回滚到最初状态。

### 8.1.1 事务的 ACID 特性

- 原子性：原子性指的是事务是一个不可分割的工作，要么全部提交，要么全部回滚。不存在中间状态。

- 一致性：一致性是指事务执行前后，数据从一个**合法性状态**变换到另一个**合法性状态**。这种状态是**语义上**的而不是语法上的，跟具体的业务有关。满足**预定的约束**的状态就叫做合法的状态。满足这个状态，数据就是一致的，不满足这个状态，数据就是不一致的。如果事务中某个操作失败了，系统就会自动撤销当前正在执行的事务，返回到事务操作之前的状态。

  ![image-20220306184049198](image-20220306184049198.png)

- 隔离性：事务的隔离性指的是一个事务的执行**不能被其他事务干扰**，即一个事务内部的操作及使用的数据对**并发**的其他事务是隔离的，并发执行的各个事务之间不能相互干扰。

  ![image-20220306185229151](image-20220306185229151.png)

- 持久性：持久性指的是一个事务一旦被提交，它对数据库的改变就是**永久性的**，接下来的其他操作和数据库故障不应该对其有任何影响。持久性是通过**事务日志**保证的，日志包括**重做日志和回滚日志**。当我们通过事务对数据进行修改时，首先会将数据库的变化信息记录到重做日志中，然后对数据库中对应的行进行修改。这样即使数据库系统崩溃，我们也可以通过重做日志重新执行，使事务具有持久性。

ACID 是事务的四大特性，在四个特性中，原子性是基础，隔离性是手段，一致性是约束条件，持久性是我们的目的。

### 8.1.2 事务的状态

MySQL 根据这些操作所执行的不同阶段把事务大致划分为几个阶段：

- 活动的：事务对应的数据库操作正在执行过程中，我们就说该事务处在活动的状态
- 部分提交的：当事务中的最后一个操作执行完成，但是由于操作都在内存中执行，并**没有刷新到磁盘**，我们就说事务处在部分提交的状态
- 失败的：当事务处在**活动的**或者**部分提交的**状态时，可能遇见某些错误，无法继续执行。或者人为停止当前事务的执行，就说该事务处于**失败的**状态。
- 中止的：如果事务执行一部分而变为**失败的**状态，那么就需要把已经修改的事务中的操作还原到事务执行前的状态。这个撤销过程称之为**回滚**。当回滚操作执行完毕时，也就是事务恢复到了执行事务之前的状态，我们就说事务处在了**中止的**状态。
- 提交的：当一个处在部分提交的状态的事务将修改过的数据都**同步到磁盘**上之后，我们就可以说事务处在了**提交的**状态。

![image-20220306205920896](image-20220306205920896.png)

由此可见，事务只有处于**提交的**或者**中止的**状态时，一个事务的生命周期才算是结束了。对于已经提交的事务，事务对数据库的修改将永久生效；对于处于中止状态的事务，该事务对数据库所做的修改都会被回滚到没有执行事务之前。

## 8.2 如何使用事务

事务分为两种：显式事务和隐式事务

### 8.2.1 显式事务

![image-20220307094416106](image-20220307094416106.png)

补充：只读事务中只是**不允许修改那些其他事务也能访问到的表**中的数据，对于**临时表**来说，由于临时表只在当前会话中可见，所以只读事务其实也是可以对临时表进行增、删、改操作的。

同时，`ROLLBACK TO [SAVEPOINT]` 回滚到某个保存点后，事务并不处于中止的或者提交的状态。还需要继续执行，直到这两个状态：提交的状态（COMMIT）或终止的状态（ROLLBACK）才会停止事务。

### 8.2.2 隐式事务

![image-20220307094439225](image-20220307094439225.png)

这样，我们写入的多条语句就算是属于同一个事务了，直到我们显示的写出 COMMIT 语句来把这个事务提交掉，或者显式的写出 ROLLBACK 语句来把这个事务回滚掉。

补充：`SET autocommit = OFF` 操作对于 DML 操作是有效的，对于 DDL 操作是无效的。

Oracle 默认不自动提交，需要手写 COMMIT 命令，而 MySQL 默认自动提交。

以下操作是不受 `autocommit` 参数控制，默认提交事务的情况：

![image-20220322102330270](image-20220322102330270.png)

![image-20220322102502773](image-20220322102502773.png)

![image-20220322102601944](image-20220322102601944.png)

### 8.2.3 使用举例：提交与回滚

![image-20220307101410848](image-20220307101410848.png)

![image-20220307101427845](image-20220307101427845.png)

![image-20220307101441175](image-20220307101441175.png)

**注意**：这里我们会看到，只是在事务开启设置了 `SET @@completion_type = 1;` 结果就和第一个处理的一样：：

这里讲解一下 MySQL 中 completion_type 的作用，这个参数实际上有三种可能：

1. `completion = 0`，这是默认情况，当我们执行 `COMMIT` 的时候会提交事务，执行下一个事务时，还需要使用 `START TRANSACTION` 或者 `BEGIN` 来开启。
2. `completion = 1`，这种情况下，当我们提交事务，相当于执行了 `COMMIT AND CHAIN`，也就是开启一个**链式事务**，即当我们提交事务之后会开启一个相同隔离级别的事务。
3. `completion = 2`，这种情况下，`COMMIT = COMMIT AND RELEASE`，也就是当我们提交后，会**自动与服务器断开连接**。

## 8.3 事务隔离级别

MySQL 是一个**客户端/服务器**架构的软件，每个客户端与服务器连上之后，都可以称为一个会话（Session）。每个客户端都可以在自己的会话中向服务器发出请求语句，一个请求语句可能是某个事务的一部分，也就是对于服务器来说可能同时处理多个事务。事务有隔离性的特性，理论上在某个事务**对某个数据进行访问**时，其他事务应该**排队**等待。但是这样对性能影响很大。我们一方面想保持事务的隔离性，另一方面又想让访问同一数据的多个事务性能尽量高一些，就要看二者如何取舍了。

### 8.3.1 数据并发问题

我们讨论的基础是建立在：访问相同数据的事务在**不保证串行执行**的情况下可能会出现的问题。

#### 8.3.1.1 脏写

对于两个事务 A、B，如果事务 A**修改了**另一个**未提交**事务 B**修改过**的数据，那就发生了脏写。

![image-20220307152317433](image-20220307152317433.png)

#### 8.3.1.2 脏读

对于两个事务 A、B，A**读取**了已经被 B**更新**但是还**没有被提交**的字段。之后若 B**回滚**，A 读取的内容就是**临时且无效**的。

![image-20220307152457109](image-20220307152457109.png)

#### 8.3.1.3 不可重复读

对于两个事务 A、B，A**读取**了一个字段，然后 B**更新**了该字段。之后 A**再次读取**同一个字段，**值就不同了**。那就意味着发生了不可重复读。

![image-20220307152920329](image-20220307152920329.png)

#### 8.3.1.4 幻读

对于两个事务 A、B，A 从一个表中**读取**一个字段，然后 B 在该表中**插入**了一些新的行。之后，如果 A**再次读取**一个表，就会多出几行。那就意味着发生了幻读。

![image-20220307153205671](image-20220307153205671.png)

![image-20220307153249824](image-20220307153249824.png)

### 8.3.2 SQL 中的四种隔离级别

上面介绍了几种并发事务执行过程中可能遇到的一些问题，这些问题有轻重缓急之分，我们给这些问题按照严重性来排一下序：

```
脏写 > 脏读 > 不可重复读 > 幻读
```

我们愿意舍弃一部分隔离性来换取一部分性能在这里就体现在：设立一些隔离级别，隔离级别越低，并发问题发生的就越多。SQL 标准中设立了 4 个隔离级别 ：

- `READ UNCOMMITTED` ：读未提交，在该隔离级别，所有事务都可以看见其他未提交事务的执行结果。不能避免脏读，不可重复读，幻读。
- `READ COMMIT` ：读已提交，满足了隔离的简单定义：一个事务只能看见已经提交事务所做的改变。可以避免脏读，但是不可重复读和幻读问题依然存在。
- `REPEATABLE READ` ：可重复读，事务 A 在读到一条数据之后，此时事务 B 对该数据进行了修改并提交。那么事务 A 读到的数据，还是原来的内容。这种情况只不能解决幻读问题。这是**MySQL 的默认隔离级别**。
- `SERIALIZABLE` ：可串行化，确保事务可以从一个表中读取相同的行。但是事务持续期间，禁止其他事务对表执行操作。可以避免所有问题。

![image-20220307154000935](image-20220307154000935.png)

### 8.3.3 MySQL 支持的四种隔离级别

![image-20220307154629565](image-20220307154629565.png)

### 8.3.4 如何设置事务的隔离级别

![image-20220307154740301](image-20220307154740301.png)

![image-20220307154752028](image-20220307154752028.png)

### 8.3.5 不同隔离级别举例

![image-20220307161808865](image-20220307161808865.png)

![image-20220307161821830](image-20220307161821830.png)

![image-20220307161833680](image-20220307161833680.png)

![image-20220307162029685](image-20220307162029685.png)

![image-20220307162731524](image-20220307162731524.png)

## 8.4 事务的常见分类

从事务理论的角度我们将事务分为以下几类：

- 扁平事务
- 带有保存点的扁平事务
- 链事务
- 嵌套事务
- 分布式事务

![image-20220307102033553](image-20220307102033553.png)

![image-20220307102116604](image-20220307102116604.png)

![image-20220307102143835](image-20220307102143835.png)

# MySQL 事务日志

事务有 4 种特性：原子性、一致性、隔离性和持久性。事务四种特性到底是基于什么机制呢？

- 事务的隔离性由**锁机制**实现。
- 事务的原子性、一致性和持久性由事务的 redo 日志和 undo 日志来保证
  - REDO LOG 称为重做日志，提供再写入操作，恢复提交事务修改的页操作，用来保证事务的持久性。
  - UNDO LOG 称为回滚日志，回滚行记录到某个特定版本，用来保证事务的原子性、一致性。

有的 DBA 会认为 UNDO 是 REDO 的逆过程，其实不然。REDO 和 UNDO 都可以视为一种**恢复操作**，但是：

- redo log：是存储引擎层（InnoDB）生成的日志，记录的是**物理级别**上的页修改操作，比如页号、偏移量、写入数据等。主要是为了保证数据的可靠性。
- undo log：是存储引擎层（InnoDB）生成的日志，记录的是**逻辑操作**日志，比如对某一行数据进行了 `INSERT` 语句操作，那么 undo log 就记录一条与之相反的 `DELETE` 操作。主要用于**事务的回滚**（undo log 记录的是每个修改操作的**逆操作**）和**一致性非锁定读**（undo log 回滚行记录到某种特定的版本——MVCC，即多版本并发控制）

相当于，redo 日志保证事务提交状态时的操作；undo 日志保证事务中止状态时的操作（对应事务的状态）

## 9.1 redo 日志

InnoDB 是以**页为单位**来管理存储空间的。在真正访问页面之前，需要把**磁盘上**的页缓存到内存的 `**Buffer Pool**` 之后才可以访问。所有变更都必须**先更新缓冲池**中的数据，然后缓冲池中的**脏页**（内存中修改，但是磁盘中没有修改的页称为脏页）会以一定频率刷入磁盘（checkPoint 机制）

### 9.1.1 为什么需要 redo 日志

一方面，缓冲池可以帮助我们消除 CPU 和磁盘之间的鸿沟，checkpoint 机制可以保证数据的最终落盘。然后由于 checkpoint 并不是**每次变更的时候就触发**的。如果事务提交之后，刚写完缓冲池，数据库宕机，那么这段数据就丢失，无法恢复。

另一方面，事务包含**持久性**的特性，对于一个已经提交的事务，在事务提交后即使系统发生了崩溃，这个事务对数据库中所做的更改也不能丢失。

保证持久性有一个简单的做法：在事务提交完成之前把该事务所修改的所有页面都刷新到磁盘，但是存在以下问题：

- 修改量与刷新磁盘的工作量不成正比：InnoDB 是以页为单位来进行磁盘 IO，我们又知道一个页默认 16KB 大小，如果只修改一个字节就要刷新 16KB 显然有些小题大做。
- 随机 IO 刷新较慢：一个事务可能包含很多语句，即使是一条语句也可能修改许多页面，这意味着事无修改刷新到磁盘时，进行大量的随机 IO，时间会很慢。

另一个解决的思路：我们没有必要在每次事务提交时就把该事务在内存中修改的全部页面刷新到磁盘，只需要把**修改了哪些东西记录一下就好**。

InnoDB 引擎的事务采用了 WAL 技术（`Write-Ahead Logging`），这种技术的思想就是先写日志，在写磁盘。只有日志写入成功，才算事务提交成功，这里的日志就是 redo log。当数据库宕机并且数据未刷新到磁盘时，可以通过 redo log 来恢复。

![image-20220307203533119](image-20220307203533119.png)

### 9.1.2 redo 日志的好处和特点

1. 好处：

   - redo 日志降低了刷盘频率
   - redo 日志占用的空间非常小

   存储表空间 id、页号、偏移量以及需要更新的值，所需要的存储空间很小，刷盘很快。

2. 特点

   - redo 日志是顺序写入磁盘的：执行事务的过程中，每执行一条语句，就可能产生若干 redo 日志，这些日志是按照**产生的顺序写入磁盘的**，也就是顺序 IO，效率比随机 IO 快。
   - 事务执行过程中，redo log 不断记录：redo log 和 bin log 的区别，除了一个是存储引擎层另一个是数据库层之外，bin log 直到一个事务提交，才会一次写入到 bin log 文件中。

### 9.1.3 redo 的组成

redo 日志可以简单分为一下两个部分：

- **重做日志的缓冲（redo log buffer）**，保存在内存中，是易失的。

  ![image-20220307204203733](image-20220307204203733.png)

  ![image-20220307204243701](image-20220307204243701.png)

- **重做日志文件（redo log file）**，保存在磁盘中，是持久的。

### 9.1.4 redo 的整体流程

![image-20220307204521609](image-20220307204521609.png)

### 9.1.5 redo log 的刷盘策略

![image-20220307204928071](image-20220307204928071.png)

![image-20220307204941557](image-20220307204941557.png)

![image-20220307205218047](image-20220307205218047.png)

也就是说，一个没有提交事务的 `redo log` 记录，也可能会刷盘。

为什么呢？

因为在事务执行过程 `redo log` 记录是会写入 `redo log buffer` 中，这些 `redo log` 记录会被后台线程刷盘。

除了后台线程每秒 `1` 次的轮询操作，还有一种情况，当 `redo log buffer` 占用的空间即将达到 `innodb_log_buffer_size` 一半的时候，后台线程会主动刷盘。

### 9.1.6 不同刷盘策略演示

![image-20220307205544167](image-20220307205544167.png)

![image-20220307205618464](image-20220307205618464.png)

![image-20220307205735467](image-20220307205735467.png)

![image-20220307205840444](image-20220307205840444.png)

![image-20220307210007421](image-20220307210007421.png)

![image-20220307210120304](image-20220307210120304.png)

个人理解：这里数值 0 应该才是效率最高的？因为提交事务不受影响，之和 1S 延迟相关，而其余的除了 1S 延迟之后，还需要在事务提交时主动刷一次，所以比 0 要慢。

### 9.1.7 写入 redo log buffer 过程

#### 9.1.7.1 补充概念：Mini-Transaction

**MySQL 把对底层页面中的一次原子访问的过程称之为一个 Mini-Transaction**，简称 mtr，比如，向某个索引对应的 B+树中插入一条记录的过程就是一个 mtr。一个所谓的 mtr 可以包含一组 redo 日志，在进行崩溃恢复时，这一组 redo 日志作为一个不可分割的整体。

![image-20220308151920733](image-20220308151920733.png)

#### 9.1.7.2 redo 日志写入 log buffer

向 log buffer 写入日志的过程是顺序的，也就是先在前面的 block 中写，当前面的 block 空间用完之后，再往后面的 block 中写。当我们想往 log buffer 中写入 redo 日志时，我们需要知道写在哪个 block 的哪个偏移量处，InnoDB 的设计者提供了一个**buf_free**的全局变量，该变量指明 redo 日志应该写到 log buffer 的哪个位置。

![image-20220308152358586](image-20220308152358586.png)

一个 mtr 的过程可能产生若干条 redo 日志，**这些 redo 日志是一个不可分割的组**，所以其实并不是没生成一条 redo 日志，就插入到 log buffer 中，而是每个 mtr 运行过程中产生的日志先暂时存在一个地方。在 mtr 结束的时候，将过程中产生的一组 redo 日志全部复制到 log buffer 中。

**即：mtr 内部必须是完全连存储在 log buffer 上的，任何一个 mtr 都不能够被分割成两部分存储在 log buffer 上；同一个事务之间的不同 mtr 存储在 log buffer 上不一定是连续的，因为事务可能是并发的。**

我们现在假设有两个名为 T1、T2 的事务，每个事务包含两个 mtr，我们命名一下：

![image-20220308152816509](image-20220308152816509.png)

![image-20220308152830342](image-20220308152830342.png)

![image-20220308153004471](image-20220308153004471.png)

有的 mtr 产生的 redo 日志非常大，比如上图的紫色部分，占用了 3 个 block 存储。

#### 9.1.7.3 redo log block 的结构图

一个 redo log block 由日志头、日志体和日志尾组成。日志头占用 12 字节，日志尾占用 8 字节。所以一个 block 真正存储空间为 492 字节。（512-12-8 = 492）

![image-20220308153415557](image-20220308153415557.png)

![image-20220308153553891](image-20220308153553891.png)

![image-20220308153511515](image-20220308153511515.png)

### 9.1.8 redo log file

#### 9.1.8.1 相关参数设置

![image-20220308153654022](image-20220308153654022.png)

在数据库实例更新比较频繁的情况下，**可以适当加大 redo log 组数和大小**。但是不推荐设置过大，在 MySQL 崩溃恢复时会重新执行 redo 日志中的记录。

#### 9.1.8.2 日志文件组

![image-20220308154151515](image-20220308154151515.png)

#### 9.1.8.3 checkpoint 

![image-20220308154320161](image-20220308154320161.png)

![image-20220308154655244](image-20220308154655244.png)

总结：write pos 随着 redo buffer 刷新到 redo log file 中而记录后移。如果 MySQL 需要恢复数据，那我们就会把 checkpoint 后移，并把这些数据恢复。如果 write pos 追上了 checkpoint，那我们就需要清空一些历史记录，因为已经记录不下这么多历史数据用来做回滚了。（这里老师讲错了？17:30——P171）

### 9.1.9 小结

![image-20220308155635637](image-20220308155635637.png)

## 9.2 undo 日志

redo log 是事务持久性的保证，undo log 是事务原子性的保证。**在事务中更新数据的前置操作其实是要先写一个 undo log**

### 9.2.1 如何理解 undo 日志

事务需要保证原子性，要么全部完成要么什么都不做。如果出现以下情况：

1. 事务执行过程中遇见各种错误，比如操作系统或者服务器的错误，甚至断电。
2. 程序员可以手动输入 `ROLLBACK` 语句结束当前事务的执行。

以上情况出现，我们需要把数据改回原先的样子，这个过程称之为**回滚**，这样就可以造成一个假象：这个事务看起来什么都没做，所以符合**原子性**要求。

所以，如果我们要对一条记录做改动时，都要把回滚的东西记录下来，比如：

- 插入一条记录，我们就记录删除操作。
- 删除一条记录，我们就需要把插入原本的信息记录。
- 修改一条记录，至少要把修改这条记录的旧值记录下来，然后回滚就变成更新旧值就可以。

MySQL 把这些为了回滚而记录的内容称之为 undo log。**查询操作不会记录 undo 日志，因为不会修改记录。**

此外，**undo log 会产生 redo log**，也就是 undo log 的产生会伴随着 redo log 的产生，这是因为**Undo log 也需要持久性的保护。**

### 9.2.2 undo 日志的作用

1. 回滚数据：undo 是逻辑日志，因此只能将数据库逻辑的恢复到原来的样子。所有修改都被逻辑地取消了，但是数据结构和页本身在回滚之后可能大不相同。
2. MVCC：MVCC 的实现是通过 undo 完成。当用户读取一行记录时，若该记录被其他事务占用，当前事务可以通过 undo 读取之前的行版本信息，以此实现非锁定读取。

### 9.2.3 undo 的存储结构

#### 9.2.3.1 回滚段与 undo 页

InnoDB 对 undo log 的管理采用段的方式，也就是**回滚段**。每个回滚段记录**1024**个 undo log segment，而在每个 undo log segment 段中进行 undo 页的申请。

![image-20220308163638330](image-20220308163638330.png)

![image-20220308163846698](image-20220308163846698.png)

![image-20220308163944907](image-20220308163944907.png)

#### 9.2.3.2 回滚段与事务

![image-20220308164136293](image-20220308164136293.png)

#### 9.2.3.3 回滚段中的数据分类

![image-20220308164400380](image-20220308164400380.png)

### 9.2.4 undo 的类型

在 InnoDB 存储引擎中，undo log 分为：

- insert undo log：指的是 insert 操作中产生的 undo log。因为 insert 记录只对事务本身可见，对其他事务不可见（这是事务隔离性的要求）。所以该 undo log 可以做事务提交后直接删除。不需要 purge 操作。
- update undo log：指的是 delete 和 update 操作产生的 undo log。该 undo log 可能需要提供 MVCC 机制，因此不能在事务提交时就进行删除。提交时放入 undo log 链表，等待 purge 线程进行最后的删除。

### 9.2.5 undo log 的生命周期

![image-20220308165044726](image-20220308165044726.png)

![image-20220308165413089](image-20220308165413089.png)

![image-20220308165421966](image-20220308165421966.png)

![image-20220308165616073](image-20220308165616073.png)

插入的数据都会生成一条 insert undo log，并且数据的回滚指针会指向它。undo log 会记录 undo log 的序号、插入主键的列和值...，那么在进行 rollback 的时候，通过主键直接把对应的数据删除即可。

![image-20220308170301442](image-20220308170301442.png)

对于更新的操作会产生 update undo log，并且会分更新主键的和不更新主键的，假设现在执行

![image-20220308170312992](image-20220308170312992.png)

![image-20220308170327323](image-20220308170327323.png)

![image-20220308170426544](image-20220308170426544.png)

![image-20220308170451458](image-20220308170451458.png)

### 9.2.6 小结

![image-20220308170607214](image-20220308170607214.png)


# 10. 锁

事务的**隔离性**由这章讲述的**锁**来实现。

### 10.1 概述

锁是计算机协调多个进程或线程并发访问某一资源的机制。保证数据的完整性和一致性。

在数据库中，除传统的计算资源（如 CPU、RAM、I/O 等）的争用以外，数据也是一种供许多用户共享的资源。为保证数据的一致性，需要对**并发操作进行控制**，因此产生了锁。同时**锁机制也为实现 MySQL 的各个隔离级别提供了保证**。锁冲突也是影响数据库并发访问性能的一个重要因素。所以锁对数据库而言显得尤其重要，也更加复杂。

### 10.2 MySQL 并发事务访问相同记录

并发事务访问相同记录的情况大致可以划分为 3 种：

#### 10.2.1 读-读情况

即并发事务相继读取相同的记录。读取操作本身并不会对记录有任何影响，所以允许这种情况的发生。

#### 10.2.2 写-写情况

即并发事务相继对相同的记录做出改动。

这种情况下会发生**脏写**的问题，任何一种隔离级别都不会允许这种问题的产生。所以多个未提交事务相继对同一条记录进行修改时，需要派对执行，这个排队过程就是锁实现的。

![image-20220308185237813](image-20220308185237813.png)

![image-20220308185251403](image-20220308185251403.png)

![image-20220308185324814](image-20220308185324814.png)

![image-20220308185339012](image-20220308185339012.png)

注意：MySQL 中，**锁和事务是相关的，一个事务就对应自己的一把锁。锁和记录无关**。

小结：

- 不加锁：意思就是不需要在内存中产生对应的锁结构，可以直接进行操作。
- 获取锁成功或加锁成功：意思就是在内存中生成了对应的锁结构，而且锁结构的 `is_waiting` 属性为 false，也就是事务可以继续执行操作
- 获取锁失败或者加锁失败：意思是在内存中生成了对应的锁结构，不过锁结构的 is_waiting 属性为 true，也就是事务需要等待，不可以继续执行操作。

#### 10.2.3 读-写或写-读情况

读-写或写-读，即一个事务进行读取操作，另一个进行改动操作。这种情况下可能发生**脏读、不可重复读、幻读**的问题。

MySQL 在 `REPEATABLE READ` 隔离级别上就已经解决了**幻读**问题。

#### 10.2.4 并发问题的解决方案

解决脏读、不可重复读、幻读的问题呢？有以下两种方案：

- 方案一：读操作利用多版本并发控制（MVCC），写操作进行**加锁**。

  所谓的 MVCC，就是生成一个 `ReadView`，通过 ReadView 找到符合条件的历史版本（历史版本由 undo 日志构建）。查询语句只能读到生成 ReadView 之前已提交事务所做的更改，在生成 ReadView 之前未提交的事务或者之后才开启的事务所作的更改是看不到的。而写操作针对的是最新版本的记录，**读记录的历史版本和改动记录的最新版本并不冲突**。

  ![image-20220309194119635](image-20220309194119635.png)

- 方案二：读、写操作都采用加锁的方式。

  脏读的产生是因为当前事务读取了另一个未提交事务写的一条记录，如果另一个事务在写记录的时候就给这条记录加锁，那么就不会产生脏读的问题。

  不可重复读的产生是因为当前事务先读取一条记录，另外一个事务做了改动并且提交了之后，当前事务再次读取就会获得不同的值。如果在当前事务读取时就给该记录进行加锁，那么另一个事务就无法修改该记录，自然也不会发生不可重复读了。

  幻读的问题产生是因为当前事务读取了一个范围的记录，然后另外的事务向该范围内插入了新记录，当前事务再次读取该范围记录时发现了新插入的新纪录。采用加锁的方式解决幻读问题有一点麻烦，因为当前事务在第一次读取记录时幻影记录并不存在，所以读取的时候加锁就有点尴尬（因为你并不知道给谁加锁）

  小结对比发现：

  - 采用 MVCC 方式的话，读-写操作彼此并不冲突，性能更高。
  - 采用加锁方式的话，读-写操作彼此需要排队执行，影响性能。

  一般情况下我们当然愿意采用 MVCC 来解决读-写操作并发执行的问题，但是业务在某些特殊情况下，要求必须采用加锁的方式执行。下面就讲解下 MySQL 中不同类别的锁。

### 10.3 锁的不同角度分类

![image-20220309194907737](image-20220309194907737.png)

#### 10.3.1 从数据操作类型划分：读锁、写锁

MySQL 实现了一个由两种类型的锁组成的锁系统，来解决针对写-写、读-写、写-读问题。这两种类型的锁通常被称为共享锁和排他锁。也叫做读锁和写锁。

- 读锁：也称为共享锁、英文用 S 表示。针对同一份数据，多个事务的读操作可以同时进行而不会互相影响，相互不阻塞的。
- 写锁：也称为排他锁、英文用 X 表示。当前写操作没有完成前，它会阻断其他写锁和读锁。这样就能确保在给定的时间里，只有一个事务能执行写入，并防止其他用户读取正在写入的同一资源。

需要注意的是对于 InnoDB 引擎来说，读锁和写锁可以加在表上，也可以加在行上。

![image-20220309195839968](image-20220309195839968.png)

##### 10.3.1.1 锁定读

![image-20220309200006352](image-20220309200006352.png)

![image-20220309200037034](image-20220309200037034.png)

##### 10.3.1.2 写操作

平常所用到的写操作无非是 `DELETE、UPDATE、INSERT` 这三种：

- `DELETE` ：对一条记录做 `DELETE` 操作，其实是先在 B+树中定位这条记录的位置，然后获取这条记录的 X 锁，再执行**delete mark**操作。我们可以把这个定位待删除记录在 B+树中位置的过程看成是一个获取**X 锁**的**锁定读**。

- `UPDATE` ：对一条记录做 `UPDATE` 操作分为三种情况：

  1. 未修改记录的键值，并且被更新的列占用的存储空间在修改前后无变化。

     则先在**B+树**中定位这条记录的位置，然后获取记录的**X 锁**，最后在原记录的位置进行操作。我们可以把这个定位待修改记录在**B+树**中位置的过程看成是一个获取**X 锁的锁定读**。

  2. 未修改记录的键值，并且至少有一个被更新的列占用的存储空间在修改前后发生变化。

     则先在**B+树**中定位这条记录的位置，然后获取记录的**X 锁**，将该记录彻底删除掉（就是把记录彻底移入垃圾链表），最后再插一条记录。这个定位待修改记录在 B+树中位置的过程看成是一个获取**X 锁的锁定读**，新插入的记录由 `INSERT` 操作提供的**隐式锁**进行保护。

  3. 修改了该记录的键值，则相当于原记录上做 `DELETE` 操作之后再来一次 `INSERT` 操作，加锁操作就需要按照 `DELETE和INSERT` 的规则进行。

- `INSERT` ：一般情况下，新插入一条记录的操作并不加锁，通过一种称之为**隐式锁**的结构来保护这条新插入的记录在本事务提交前不被别的事务访问。

#### 10.3.2 从数据操作的粒度划分：表级锁、页级锁、行锁

管理锁是一种很消耗资源的事情。数据库系统为了在高并发响应和系统性能两方面进行平衡，就产生了**锁粒度**的概念。

##### 10.3.2.1 表锁

表锁会锁定整张表，是 MySQL 中最基本的锁策略，**不依赖于存储引擎**，并且表锁是开销最小的策略，**可以避免死锁问题**。但是并发效率很低。

1. 表级别的 S 锁、X 锁

   在对某个表执行 `SELECT、INSERT、DELETE、UPDATE` 语句时，InnoDB 存储引擎是不会为这个表添加表级别的 S 锁或者 X 锁的。在对某个表执行一些诸如 `ALTER TABLE 、 DROP TABLE` 这类的 DDL 语句时，其他事务对这个表并发执行诸如 `SELECT、INSERT、DELETE、UPDATE` 的语句会发生阻塞。同理，某个事务中对某个表执行 `SELECT、INSERT、DELETE、UPDATE` 语句时，在其他会话中对这个表执行 DDL 语句也会发生阻塞。这个过程其实是通过在 server 层使用一种称之为元数据锁 （英文名：Metadata Locks，简称 MDL）的结构来实现的。

   一般情况下，不会使用 InnoDB 存储引擎提供的表级别的 S 锁和 X 锁。只会在一些特殊情况下，比方说崩溃恢复过程中用到。比如，在系统变量 autocommit=0，innodb_table_locks = 1 时，手动获取 InnoDB 存储引擎提供的表 t 的 S 锁或者 X 锁可以这么写：

   - `LOCK TABLES t READ` ：InnoDB 存储引擎会对表 t 加表级别的 S 锁。
   - `LOCK TABLES t WRITE` ：InnoDB 存储引擎会对表 t 加表级别的 X 锁。

   不过尽量避免在使用 InnoDB 存储引擎的表上使用 `LOCK TABLES` 这样的手动锁表语句，它们并不会提供什么额外的保护，只是会降低并发能力而已。InnoDB 的厉害之处还是实现了更细粒度的行锁，关于 InnoDB 表级别的 S 锁和 X 锁大家了解一下就可以了。

   ![image-20220309202126629](image-20220309202126629.png)

2. 意向锁（intention lock）

   InnoDB 支持多粒度锁，他允许行级锁和表级锁共存。而**意向锁就是其中的一种表锁**。

   - 意向锁的存在是为了协调行锁和表锁的关系，支持多粒度（表锁和行锁）的锁并存。
   - 意向锁是一种**不与行级锁冲突的表级锁**。
   - 表明**某个事务正在某些行持有了锁或该事务准备去持有锁**

   意向锁分为两种：

   - 意向共享锁：事务有意向对表中的某些行加共享锁（S 锁）

     ```sql
     -- 事务要获取某系行的S锁，必须先获得表的IS锁
     SELECT column From table ... LOCK IN SHARE MODE;
     ```

   - 意向排它锁：事务有意向对表中的某些行加排它锁（X 锁）

     ```SQL
     -- 事务要获取某些行的X锁，必须先获得表的IX锁
     SELECT column From table ... FOR UPDATE;
     ```

   注意：意向锁是由存储引擎自己维护的，用户无法手动操作意向锁。在为数据行加共享/排他锁之前，InnoDB 会先获取**该数据行所在数据表对应的意向锁。**

   ![image-20220309203130462](image-20220309203130462.png)

   ![image-20220309203218453](image-20220309203218453.png)

   **意向锁不会与行级的共享 / 排他锁互斥**！正因为如此，**意向锁并不会影响到多个事务对不同数据行加排他锁时的并发性**（不然我们直接用普通的表锁就行了）

   结论：

   - InnoDB 支持多粒度锁，特定场景下，行级锁可以与表级锁共存（比如某行加上了写锁，但是整体加上一个意向锁，意向锁是表锁）
   - 意向锁之间互不排斥，但除了 IS 与 S 兼容外，意向锁会与共享锁 / 排他锁互斥（这里指的是表级别的 S 和 X 锁）
   - IX，IS 是表级锁，不会和行级的 X，S 锁发生冲突。只会和表级的 X，S 发生冲突。
   - 意向锁在保证并发性的前提下，实现了**行锁和表锁共存**且**满足事务隔离性**的要求。

3. 自增锁（AUTO-INC 锁）

   在使用 MySQL 过程中，我们可以为表的某个列添加 `AUTO_INCREMENT` 属性。举例：

   ```SQL
   CREATE TABLE `teacher` (
   `id` int NOT NULL AUTO_INCREMENT,
   `name` varchar(255) NOT NULL,
   PRIMARY KEY (`id`)
   ) ENGINE=InnoDB DEFAULT CHARSET=utf8mb4 COLLATE=utf8mb4_0900_ai_ci;
   ```

   由于这个表的 id 字段声明了 `AUTO_INCREMENT`，意味着在书写插入语句时不需要为其赋值。

   上面插入数据只是一种简单的插入模式，所有插入数据的方式一共分为三类，分别是：`Simple inserts、Bulk inserts、Mixed-mode inserts`。

   - `Simple inserts`（简单插入）

     可以预先确定要插入的行数（当语句被初始处理时）的语句。包括没有嵌套子查询的单行和多行 `INSERT...VALUES()` 和 `REPLACE` 语句。

   -  `Bulk inserts`（批量插入）

     事先不知道要插入的行数（和所需自动递增值的数量）的语句。比如 `INSERT ... SELECT` ，`REPLACE ... SELECT` 和 `LOAD DATA` 语句，但不包括纯 `INSERT`。InnoDB 在每处理一行，为 `AUTO_INCREMENT` 列分配一个新值。

   -  `Mixed-mode inserts`（混合模式插入）

     这些是 `Simple inserts` 语句但是指定部分新行的自动递增值。例如 `INSERT INTO teacher (id,name) VALUES (1,'a'), (NULL,'b'), (5,'c'), (NULL,'d');` 只是指定了部分 id 的值。另一种类型的“混合模式插入”是 `INSERT ... ON DUPLICATE KEY UPDATE`。

   对于上面数据插入的案例，MySQL 中采用了**自增锁**的方式来实现，**AUTO-INC 锁是向含有 AUTO_INCREMENT 列的表中插入数据时需要获取的一种特殊的表级锁**。在执行插入语句时就在表级别加一个 AUTO-INC 锁，在该语句执行结束后，再把 AUTO-INC 锁释放掉。一个事务在 AUTO-INC 锁的过程中，其他事务的插入语句都要被阻塞，可以保证语句中分配的值都是连续的。正因为如此，当我们向一个有 AUTO_INCREMENT 关键字的主键插入值的时候，每条语句都要对这个表锁进行竞争。这样的并发潜力其实是很低下的，所以 InnoDB 通过 innodb_autoinc_lock_mode 的不同取值来提供不同的锁定机制，来提供 SQL 语句的可伸缩性和性能。

   ![image-20220309204321694](image-20220309204321694.png)

4. 元数据锁（MDL 锁）

   MySQL5.5 引入了 meta data lock，简称 MDL 锁，属于表锁范畴。MDL 的作用是，保证读写的正确性。比如，如果一个查询正在遍历一个表中的数据，而执行期间另一个线程对这个表结构做变更，增加了一列，那么查询线程拿到的结果跟表结构对不上，肯定是不行的。

   因此，**当对一个表做增删改查操作的时候，加 MDL 读锁；当要对表做结构变更操作的时候，加 MDL 写锁**。

   MDL 读锁之间不互斥，因此你可以有多个线程同时对一个表进行增删改查，读写锁之间、写锁之间是互斥的，用来保证变更表结构操作的安全性。解决了 DML 和 DDL 操作之间一致性的问题。**不需要显式使用**，在访问一个表的时候会被自动加上。

   **注意！上述描述只是针对 MDL 锁来描述的，MDL 锁是用来保证 DML 和 DDL 操作之间的一致性问题！但是实际上多个线程对一个表进行增删改查操作其实会加很多其他的锁！这些锁之间会让他们之间变成阻塞状态，保证数据的一致性问题。**

   ![image-20220309205401112](image-20220309205401112.png)

##### 10.3.2.2 行锁

行锁也称为记录锁，就是锁住某一行。MySQL 服务器层并没有实现行锁机制，**行级锁只在存储引擎层实现**。

优点：锁定力度小，发生锁冲突概率低，可以实现的并发度高。

缺点：对于锁的开销比较大，加锁比较慢，容易出现死锁现象。

InnoDB 与 MyISAM 的最大不同有两点：一是支持事务；二是采用了行级锁。

1. 记录锁（Record Locks）

   记录锁也就是仅仅把一条记录锁上，官方的类型名称为：`LOCK_REC_NOT_GAP`。比如我们把 id 值为 8 的那条记录加一个记录锁的示意图如图所示。仅仅是锁住了 id 值为 8 的记录，对周围的数据没有影响。

   ![image-20220309205800981](image-20220309205800981.png)

   记录锁也是有 S 锁和 X 锁之分的，称之为 S 型记录锁和 X 型记录锁

   - 当一个事务获取了一条记录的 S 型记录锁后，其他事务也可以继续获取该记录的 S 型记录锁，但不可以继续获取 X 型记录锁；
   - 当一个事务获取了一条记录的 X 型记录锁后，其他事务既不可以继续获取该记录的 S 型记录锁，也不可以继续获取 X 型记录锁。

2. 间隙锁（Gap Locks）

   MySQL 在 `REPEATABLE READ` 隔离级别下是可以解决幻读问题的，解决方案有两种，**可以使用 MVCC 方案解决，也可以采用加锁方案解决。**但是在使用加锁方案解决时有个大问题，就是事务在第一次执行读取操作时，那些幻影记录尚不存在，我们无法给这些幻影记录加上记录锁。InnoDB 提出了一种称之为 Gap Locks 的锁，官方的类型名称为：`LOCK_GAP`，我们可以简称为 gap 锁。比如，把 id 值为 8 的那条记录加一个 gap 锁的示意图如下。

   ![image-20220309210056326](image-20220309210056326.png)

   虽然有共享 gap 锁和独占 gap 锁这样的说法，但是他们起到的作用是相同的。而且如果对一条记录加了 gap 锁（无论是共享 gap 锁还是独占 gap 锁），**都不会限制其他事物对这条记录加记录锁或者继续加 gap 锁**。

   ![image-20220309210333632](image-20220309210333632.png)

   **注意：间隙锁是可能会出现死锁的，其实应该准确来说，所有的行级锁都是可能出现死锁的！！**

3. 临键锁（Next-Key Locks）

   有时候我们既想锁住某条记录，又想阻止其他事务在该记录前边的间隙插入新记录，所以 InnoDB 就提出了一种称之为 Next-Key Locks 的锁，官方的类型名称为：`LOCK_ORDINARY`，我们也可以简称为 next-key 锁。Next-Key Locks 是在存储引擎 innodb 、事务级别在可重复读的情况下使用的数据库锁，innodb 默认的锁就是 Next-Key locks。

   ![image-20220309210728997](image-20220309210728997.png)

   临键锁的本质就是一个记录锁和一个 gap 锁的合体，可以在保护行记录的同时，阻止别的事务将新纪录插入被保护记录前面的间隙。

   ```sql
   begin;
   select * from student where id <=8 and id > 3 for update;
   ```

4. 插入意向锁（Insert Intention Locks）

   我们说一个事务在插入一条记录时需要判断一下插入位置是不是被别的事务加了 gap 锁（next-key 锁也包含 gap 锁），如果有的话，插入操作需要等待，直到拥有 gap 锁的那个事务提交。但是**InnoDB 规定事务在等待的时候也需要在内存中生成一个锁结构**，表明有事务**想在某个间隙中插入新记录**，但是现在在等待。InnoDB 就把这种类型的锁命名为 `Insert Intention Locks`，官方的类型名称为：`LOCK_INSERT_INTENTION`，我们称为插入意向锁。插入意向锁是一种 Gap 锁，不是意向锁，在 insert 操作时产生。

   插入意向锁是在插入一条记录行前，由 `INSERT` 操作产生的一种间隙锁。该锁用于表示插入意向，当多个事务在同一区间插入位置不同的多条数据时，事务之间不需要相互等待。每个事务在获取插入行上独占的锁前，都会获取这个区间之间的间隙锁，但是因为数据行之间并不冲突，所以两个事物之间不会产生冲突（阻塞等待），总的来说，插入意向锁的特性可以分成两个部分：

   - 插入意向锁是一种特殊的间隙锁——间隙锁可以锁定开区间内的部分记录。
   - 插入意向锁之间互不排斥，所以即使多个事务在同一区间插入多条记录，只要记录本身（主键、唯一索引）不冲突，那么事务之间就不会出现冲突等待。

   注意：虽然插入意向锁中含有意向锁三个字，但是他并不属于意向锁而属于间隙锁，因为**意向锁是表锁而插入意向锁是行锁**。

   **事实上插入意向锁并不会阻止别的事务继续获取该记录上任何类型的锁。**

   ![image-20220309212101186](image-20220309212101186.png)

##### 10.3.2.3 页锁

页锁就是在页的粒度上进行锁定，锁定的数据资源比行锁要多，因为一个页中可以有多个行记录。当我们使用页锁的时候，会出现数据浪费的现象，但这样的浪费最多也就是一个页上的数据行。页锁的开销介于表锁和行锁之间，会出现死锁。锁定粒度介于表锁和行锁之间，并发度一般。

每个层级的锁数量是有限制的，因为锁会占用内存空间，锁空间的大小是有限的。当某个层级的锁数量超过了这个层级的阈值时，就会进行锁升级。锁升级就是用更大粒度的锁替代多个更小粒度的锁，比如 InnoDB 中行锁升级为表锁，这样做的好处是占用的锁空间降低了，但同时数据的并发度也下降了。

#### 10.3.3 从对待锁的态度划分：乐观锁、悲观锁

乐观锁和悲观锁并不是锁，而是锁的**设计思想**。

##### 10.3.3.1 悲观锁

悲观锁是一种思想，对数据被其他事务的修改持保守态度，会通过数据库自身的锁机制来实现，从而保证数据的排他性。

悲观锁总是假设最坏的情况，每次去拿数据的时候都认为别人会修改，所以每次在拿数据的时候都会上锁，这样别人想拿这个数据就会阻塞直到它拿到锁（共享资源每次只给一个线程使用，其它线程阻塞，用完后再把资源转让给其它线程）。比如行锁，表锁等，读锁，写锁等，都是在做操作之前先上锁，当其他线程想要访问数据时，都需要阻塞挂起。Java 中 `synchronized` 和 `ReentrantLock` 等独占锁就是悲观锁思想的实现。

`select ... for update` 是 MySQL 中的悲观锁。**注意！`select ... for update` 语句执行过程中，所有被扫描的行都会被锁上，因此在 MySQL 中用悲观锁必须确定使用了索引，而不是全表扫描，否则将会把整个表锁住。**

悲观锁使用的场景并不是很多，因为锁机制来实现的过程中，对数据库性能的开销影响很大。

##### 10.3.3.2 乐观锁

乐观锁认为对同一数据的并发操作不会总发生，属于小概率事件，不用每次都对数据上锁，但是在更新的时候会判断一下在此期间别人有没有去更新这个数据，**也就是不采用数据库自身的锁机制，而是通过程序来实现**。在程序上，我们可以采用**版本号机制或者 CAS 机制实现**。乐观锁适用于多读的应用类型，这样可以提高吞吐量。在 Java 中 `java.util.concurrent.atomic` 包下的原子变量类就是使用了乐观锁的一种实现方式：CAS 实现的。

1. 乐观锁的版本号机制：

   在表中设计一个版本字段 `version`，第一次读的时候，会获取 `version` 字段的取值。然后对数据进行更新或删除操作时，会执行 `UPDATE ... SET version=version+1 WHERE version=version`。此时如果已经有事务对这条数据进行了更改，修改就不会成功。

2. 乐观锁的时间戳机制：

   时间戳和版本号机制一样，也是在更新提交的时候，将当前数据的时间戳和更新之前取得的时间戳进行比较，如果两者一致则更新成功，否则就是版本冲突。

你能看到**乐观锁就是程序员自己控制数据并发操作的权限**，基本是通过给数据行增加一个戳（版本号或者时间戳），从而证明当前拿到的数据是否最新。

##### 10.3.3.3 两种锁的适用场景

从这两种锁的设计思想中，我们总结一下乐观锁和悲观锁的适用场景：

1. **乐观锁**适合**读操作多的场景，相对来说写操作比较少**。他的优点在于**程序实现**，**不存在死锁**问题，不过适用场景也会相对乐观，因为他阻止不了除了程序以外的数据库操作。
2. **悲观锁**适合**写操作多的场景**，因为写的操作具有**排他性**。采用悲观锁的方式，可以在数据库层面阻止其他事务对该数据的操作权限，**防止读-写和写-写的冲突**。（但是有可能会出现死锁问题）

我们把乐观锁和悲观锁总结如下：

![image-20220310100845608](image-20220310100845608.png)

#### 10.3.4 按照加锁的方式划分：显式锁和隐式锁

##### 10.3.4.1 隐式锁

一个事务在执行 `INSERT` 操作时，如果即将插入的间隙已经被其他事务加了 gap 锁，那么本次 `INSERT` 操作会阻塞，并且当前事务会在该间隙上加一个插入意向锁。否则一般情况下，`INSERT` 操作是不加锁的。如果一个事务首先插入了一条记录（此时并没有在内存生成与该记录关联的锁结构），然后另一个事务：

- 立即使用 `SELECT ... LOCK IN SHARE MODE` 语句读取记录，也就是获取记录的 S 锁，或者使用 `SELECT ... FOR UPDATE` 语句获取记录的 X 锁，怎么办？这种情况可能会产生**脏读**问题。
- 立即修改这条记录，也就是获取这条记录的 X 锁，怎么办？这种情况可能产生**脏写**问题。

这时候我们前面提过的**事务 id**就要起作用了。我们把聚簇索引和二级索引中的记录分开看一下：

- 情景一：对于聚簇索引记录来说，有一个 `trx_id` 隐藏列，该隐藏列记录着最后改动该记录的**事务 id**。那么如果在当前事务中新插入一条聚簇索引记录后，该记录的 `trx_id` 隐藏列代表的的就是**当前事务的事务 id**。**如果其他事务**此时想**对该记录添加 S 锁或者 X 锁**时，首先会看一下**该记录的 `trx_id` 隐藏列代表的事务是否是当前的活跃事务**，如果是的话，那么就帮助当前事务创建一个 X 锁 （**也就是新来的其他事务为当前事务创建一个锁结构**，`is_waiting` 属性是 false，表示当前事务可以修改，不在等待状态），然后自己进入等待状态 （**也就是新来的其他事务为自己也创建一个锁结构**，`is_waiting` 属性是 true，表示我自己（新来的事务）是不能修改的，处于等待状态）
- 对于二级索引记录来说，本身并没有 `trx_id` 隐藏列，但是在二级索引页面的 `Page Header` 部分有一个 `PAGE_MAX_TRX_ID` 属性，该属性代表对该页面做改动的最大的**事务 id**，如果 `PAGE_MAX_TRX_ID` 属性值**小于当前最小的活跃事务 id**，**那么说明对该页面做修改的事务都已经提交了**(为什么？因为当前最小的活跃事务 id 就意味着是**目前活跃中最不活跃的事务**，改动最大的事务 id 已经小于最不活跃的事务 id，那证明之前的事务已经结束了，**已经小于目前活跃中最不活跃的事务的事务了，就可以认为最近所做的操作已经和之前改动的事务无关了**)，否则就需要在页面中定位到对应的二级索引记录，然后回表找到它对应的聚簇索引记录，然后再重复情景一的做法。

即：一个事务对新插入的记录可以不显式的加锁（生成一个锁结构），但是由于事务 id 的存在，相当于加了一个**隐式锁**。**别的事务在对这条记录加 X 锁或者 S 锁时**，由于隐式锁的存在，**会帮助当前事务生成一个锁结构**，**然后自己再生成一个锁结构**后进入等待状态。隐式锁是一种延迟加锁的机制，从而减少加锁的数量。

隐式锁在实际内存对象中并不含有这个锁信息。只有当产生锁等待时，隐式锁转化为显式锁。

隐式锁的逻辑过程如下：

- A.InnoDB 的每条记录中都一个隐含的 `trx_id` 字段，这个字段存在于聚簇索引的 B+Tree 中。
- B.在操作一条记录前（只要对记录操作，就会进行隐式锁判断），首先根据记录中的 `trx_id` 检查该事务是否是活动的事务 (未提交或回滚)。如果是活动的事务，**首先将隐式锁转换为显式锁 (就是为该事务添加一个锁，锁的目标是 `INSERT` 对应的字段)**。
- C.**检查是否有锁冲突（如果操作不是 `INSERT` 的这一行数据就没有冲突）**，如果有冲突，创建锁，并设置为 `waiting` 状态。如果没有冲突不加锁，跳到 E。
- D.等待加锁成功，被唤醒，或者超时。
- E.写数据，并将自己的 `trx_id` 写入 `trx_id` 字段。

##### 10.3.4.2 显式锁

通过特定的语句进行加锁，我们一般称之为显式加锁，例如：

显式加共享锁：

```sql
select .... lock in share mode
```

显式加排他锁：

```sql
select .... for update
```

#### 10.3.5 其他锁之：全局锁

全局锁就是**对整个数据库实例加锁**，当你需要让整个库处于只读状态时，可以使用这个命令，之后其他线程的以下语句会被阻塞：数据更新语句（数据的增删改查）、数据定义语句（包括建表、修改表机构）和更新类事物的提交语句。**全局锁的典型使用场景是：做全库逻辑备份。**

全局锁的命令：

```sql
Flush tables with read lock
```

#### 10.3.6 其他锁之：死锁

JUC 已经拿下了。死锁的关键在于：两个或以上的 session 加锁的顺序不一致。

![image-20220310141026410](image-20220310141026410.png)

死锁的解决策略：

- 方式 1：等待，直到超时。

  即两个事务互相等待时，当一个事务等待时间超过设置的阈值时，就将其回滚。在 InnoDB 中，这个超时时间可以通过参数 `innodb_lock_wait_timeout` 来设置。

- 方式 2：使用死锁检测进行死锁处理。

  InnoDB 还提供了一种叫做**wait-for graph**的算法来主动进行死锁检测，每当加锁请求无法立即满足而进入等待时，这个算法就会被触发。

  ![image-20220310141951986](image-20220310141951986.png)

  ![image-20220310142008416](image-20220310142008416.png)

  一旦检测到有回路，即代表有死锁情况产生。这时候 InnoDB 存储引擎会选择**回滚 undo 量最小的事务**，让其他事务继续执行。

  缺点：每个新的被阻塞线程，都要判断是不是由于自己的加入导致了死锁，这个时间复杂度为 O (n²)

  ![image-20220310142306136](image-20220310142306136.png)

避免死锁的方法（MySQL 角度）：

- 合理设计索引，使业务 SQL 尽可能通过索引定位更少的行，减少锁竞争。
- 调整业务逻辑 SQL 执行顺序，避免 `update、delete` 长时间持有锁的 SQL 在事务前面。
- 避免大事务，尽量将大事务拆分成多个小事务来处理。
- 在并发比较高的系统中，不要显式加锁，特别是不要在事务里显式加锁。
- 降低隔离级别，如果业务允许，将隔离级别调低也是较好的选择。

### 10.4 锁的内存结构

是不是一个事务对多条记录加锁，就要创建多个锁结构呢？比如：

```sql
# 事务T1
SELECT * FROM user LOCK IN SHARE MODE;
```

理论上创建多个锁结构没有问题，但是对一个事务如果每条数据获取一个锁，是很崩溃的事情。所以决定在对不同记录加锁时，如果符合下边这些条件的记录会放到一个锁结构中：

- 在同一个事务中进行加锁操作
- 被加锁的记录在同一个页面中
- 加锁的类型是一样的
- 等待状态是一样的

InnoDB 存储引擎中的锁结构如下：

![image-20220310143144303](image-20220310143144303.png)

锁结构解析：

1. 锁所在的事务信息：

   不论是表锁还是行锁，都是在事务执行过程中生成的，**哪个事务生成了这个锁结构，这里就记录这个事务的信息**。此**锁所在的事务信息在内存结构中只是一个指针**，通过指针可以找到内存中关于该事务的更多信息，比方说事务 id 等。

2. 索引信息：

   对于**行锁**来说，需要记录一下加锁的记录是属于哪个索引的。这里也是一个指针。

3. 表锁/行锁信息：

   表锁结构和行锁结构在这个位置的内容是不同的：

   - 表锁：记载着是对哪个表加的锁，还有其他的一些信息。
   - 行锁：记载了三个重要信息：
     - `Space ID` ：记录所在表空间
     - `Page Number` ：记录所在页号
     - `n_bits` ：对于行锁来说，一条记录就对应一个比特位，一个页面中包含很多记录，**用不同的比特位来区分到底是哪一条记录加了锁**。为此在行锁结构的末尾放置了一堆比特位，这个 `n_bits` 属性代表使用了多少比特位。（n_bits 的值一般都比页面中记录条数多一些，主要是为了之后在页面中插入了新纪录后页不至于重新分配锁结构）

4. `type_mode` ：

   ![image-20220310144939646](image-20220310144939646.png)

   ![image-20220310144953340](image-20220310144953340.png)

5. 其他信息：

   为了更好的管理系统运行过程中生成的各种锁结构而设计了各种哈希表和链表。

6. 一堆比特位：

   如果是**行锁结构**的话，在结构末尾放置了一堆比特位。比特位的数量是由上面提高的 n_bits 属性表示的。**InnoDB 数据页中的每条记录在记录头信息中都包含一个 `heap_no` 属性**，伪记录 Infimum 的 `heap_no` 值为 0（对应数据页中的最小值），Supremum 的 `heap_no` 值为 1（对应数据页中的最大值），之后每插入一条记录，`heap_no` 值就增 1（2.2.2.2 小节中提到过，最小值和最大值在每一页的最前面，分别对应 `heap_no` 为 0 和 1，之后每插入一条记录，这个值就会依次递增，**如果不按照顺序添加，这个值应该也会进行调整？**）。**锁结构最后的一堆比特位就对应着一个页面中的记录，一个比特位映射一个 `heap_no`，即一个比特位映射到页内的一条记录。**

### 10.5 锁监控

关于 MySQL 锁的监控，我们一般可以通过检查 `InnoDB_row_lock` 等状态变量来分析系统上的行锁的争夺情况

![image-20220310145321500](image-20220310145321500.png)

![image-20220310145616527](image-20220310145616527.png)

![image-20220310145632943](image-20220310145632943.png)

![image-20220310145655096](image-20220310145655096.png)

![image-20220310145708472](image-20220310145708472.png)

![image-20220310145727095](image-20220310145727095.png)

# 11. MVCC（多版本并发控制）

### 11.1 什么是 MVCC

**MVCC 是通过数据行的多个版本管理（多个版本通过 undo log 实现，管理通过 ReadView 实现）来实现数据库的并发控制**。这项技术使得 InnoDB 的事务隔离级别下执行**一致性读**操作有了保证。就是为了查询一些正在被另一个事务更新的行，并且可以看到它们被更新之前的值，这样在做查询的时候就不用等待另一个事务释放锁。

在不同的 DBMS 中 MVCC 的实现方式可能是不同的，也不是普遍使用的。这里讲解 InnoDB 中的 MVCC 实现机制（MySQL 其他的存储引擎并不支持它）

### 11.2 快照读和当前读

MVCC 在 MySQL InnoDB 中的实现主要是为了提高数据库并发性能，用更好的方式去处理读-写冲突，做到即使有读写冲突时，也能做到不加锁，非阻塞并发读，**而这个读指的就是快照读 , 而非当前读**。**当前读实际上是一种加锁的操作，是悲观锁的实现。而 MVCC 本质是采用乐观锁思想的一种方式。**

#### 11.2.1 快照读

快照读又叫一致性读，读取的是快照数据。不加锁的简单的 `SELECT` 都属于快照读，即不加锁的非阻塞读；比如这样：

```SQL
SELECT * FROM player WHERE ...
```

之所以出现快照读的情况，是基于提高并发性能的考虑，快照读的实现是基于 MVCC，它在很多情况下，避免了加锁操作，降低了开销。

**既然是基于多版本，那么快照读可能读到的并不一定是数据的最新版本，而有可能是之前的历史版本**。

快照读的前提是隔离级别不是串行级别，串行级别下的快照读会退化成当前读。

#### 11.2.2 当前读

当前读读取的是记录的最新版本（最新数据，而不是历史版本的数据），读取时还要保证其他并发事务不能修改当前记录，会对读取的记录进行加锁。**加锁的 `SELECT`，或者对数据进行增删改都会进行当前读**。比如：

```SQL
SELECT * FROM student LOCK IN SHARE MODE; # 共享锁
SELECT * FROM student FOR UPDATE; # 排他锁
INSERT INTO student values ... # 排他锁
DELETE FROM student WHERE ... # 排他锁
UPDATE student SET ... # 排他锁
```

### **11.3 复习**

#### 11.3.1 再谈隔离级别

![image-20220310151536818](image-20220310151536818.png)

在 MySQL 中，**默认的隔离级别是可重复读**，解决脏读和不可重复读的问题。

不仅如此，MySQL 使用了 MVCC 机制，可以在不采用锁机制的情况下，通过乐观锁的方式来解决不可重复读和幻读问题！它可以在大多数情况下代替行级锁，降低系统的开销。

![image-20220310151738985](image-20220310151738985.png)

#### 11.3.2 隐藏字段、undo log 版本链

回顾一下 undo 日志的版本链，对于使用 InnoDB 存储引擎的表来说，它的**聚簇索引记录中都包含两个必要的隐藏列**。

- `trx_id` ：每次一个事务对某条聚簇索引记录进行改动时，都会把该事务的**事务 id**赋值给 `trx_id` 隐藏列
- `roll_pointer` ：**每次对某条聚簇索引记录进行改动时，都会把旧的版本写入到 undo 日志中**，然后这个隐藏列就相当于一个指针，可以通过它来找到该记录修改前的信息。

 ![image-20220310152135937](image-20220310152135937.png)

![image-20220310152349984](image-20220310152349984.png)

![image-20220310152413171](image-20220310152413171.png)

### 11.4 MVCC 实现原理之 ReadView

**MVCC 的实现依赖于：隐藏字段、undo log、ReadView**

#### 11.4.1 什么是 ReadView

在 MVCC 机制中，多个事务对同一个行记录进行更新会产生多个历史快照，这些历史快照保存在 undo log 里，如果一个事务想要查询这个行记录，需要读取哪个版本的行记录呢？这是就需要用到 ReadView 了，它帮我们解决了行的可见性问题。

ReadView 就是事务在使用 MVCC 机制进行快照读操作时产生的读视图（**ReadView 和事务是一对一的）**。当事务启动时，会生成数据库系统当前的一个快照，**InnoDB 为每个事务构造了一个数组，用来记录并维护系统当前活跃事务的 id**（活跃指的是，事务启动了但是没有提交）

#### 11.4.2 设计思路

使用 `READ UNCOMMITTED` 隔离级别的事务，由于可以读到未提交事务修改过的记录，所以直接读取记录的最新版本就好了。

使用 `SERIALIZABLE` 隔离级别的事务，InnoDB 规定使用加锁的方式来访问记录。

使用 `READ COMMITTED` 和 `REPEATABLE READ` 隔离级别的事务，都**必须保证读到已经提交了的事务修改过的记录**。假如另一个事务已经修改了记录但是尚未提交，是不能直接读取最新版本的记录的，**核心问题就是需要判断一下版本链中的哪个版本是当前事务可见的**，这是 ReadView 要解决的主要问题。

这个 ReadView 中主要包含 4 个比较重要的内容，分别如下：

1. `creator_trx_id` ：创建这个 ReadView 的事务 ID

   说明：只有在对表中的记录做改动时（执行 `INSERT、DELETE、UPDATE` 这些语句时）才会为事务分配事务 id，否则在一个只读事务中的事务 id 值都默认为 0。

2. `trx_ids` ：表示在生成 ReadView 时**当前系统中活跃的读写事务的事务 id 列表**。

3. `up_limit_id` ：**活跃的事务中最小的**事务 ID。

4. `low_limit_id` ：表示生成 ReadView 时**系统中应该分配给下一个事务的 id 值**。`low_limit_id` 是系统最大的事务 id 值，这里要注意是系统中的事务 id，需要区别于正在活跃的事务 ID。

   注意：`low_limit_id` 并不是 `trx_ids` 中的最大值，事务 id 是递增分配的。比如，现在有 id 为 1， 2，3 这三个事务，之后 id 为 3 的事务提交了。那么一个新的读事务在生成 ReadView 时，`trx_ids` 就包括 1 和 2，`up_limit_id` 的值就是 1，`low_limit_id` 的值就是 4。

![image-20220310153933288](image-20220310153933288.png)

#### 11.4.3 ReadView 的规则

有了这个 ReadView，这样在访问某条记录时，只需要按照下边的步骤判断记录的某个版本是否可见。

- 如果被访问版本的 `trx_id` 属性值与 ReadView 中的 `creator_trx_id` 值相同，意味着当前事务在访问它自己修改过的记录，所以该版本可以被当前事务访问。
- 如果被访问版本的 `trx_id` 属性值小于 ReadView 中的 `up_limit_id` 值，表明生成该版本的事务在当前事务生成 ReadView 前已经提交，所以该版本可以被当前事务访问。
- 如果被访问版本的 `trx_id` 属性值大于或等于 ReadView 中的 `low_limit_id` 值，表明生成该版本的事务在当前事务生成 ReadView 后才开启，所以该版本不可以被当前事务访问。
- 如果被访问版本的 `trx_id` 属性值在 ReadView 的 `up_limit_id` 和 `low_limit_id` 之间，那就需要判断一下 `trx_id` 属性值是不是在 `trx_ids` 列表中。
  - 如果在，说明创建 ReadView 时生成该版本的事务还是活跃的，该版本不可以被访问。
  - 如果不在，说明创建 ReadView 时生成该版本的事务已经被提交，该版本可以被访问.

#### 11.4.4 MVCC 整体操作流程

了解了这些概念之后，我们来看下当查询一条记录的时候，系统如何通过 MVCC 找到它：

1. 首先获取事务自己的版本号，也就是事务 ID；
2. 获取 ReadView；
3. 查询得到的数据，然后与 ReadView 中的事务版本号进行比较；
4. 如果不符合 ReadView 规则，就需要从 Undo Log 中获取历史快照；
5. 最后返回符合规则的数据。

如果某个版本的数据对当前事务不可见的话，那就顺着版本链找到上一个版本的数据，继续按照上边的步骤判断可见性，直到版本链中的最后一个版本。如果最后一个版本也不可见的话，那么就意味着该记录对该事务完全不可见，查询结果不包含该记录。

在 InnoDB 中，MVCC 是通过 Undo Log + ReadView 进行数据读取，Undo Log 保存历史快照，而 ReadView 规定帮助我们判断当前版本的数据是否可见。

**在隔离级别为读已提交（Read Committed）时，一个事务中的每一次 `SELECT` 查询都会重新获取一次 ReadView。**

**当隔离级别为可重复读的时候，就避免了不可重复读，这是因为一个事务只在第一次 `SELECT` 的时候会获取一次 ReadView，而后面所有的 `SELECT` 都会复用这个 Read View**

![image-20220310155307973](image-20220310155307973.png)

### 11.5 举例说明

![image-20220310155712085](image-20220310155712085.png)

#### 11.5.1 `READ COMMITTED` 隔离级别下

**`READ COMMITTED` ：每次读取数据前都生成一个 ReadView**

![image-20220310155852522](image-20220310155852522.png)

![image-20220310160250847](image-20220310160250847.png)

![image-20220310160403164](image-20220310160403164.png)

![image-20220310160414462](image-20220310160414462.png)

![image-20220310161229241](image-20220310161229241.png)

![image-20220310161314179](image-20220310161314179.png)

#### 11.5.2 `REPEATABLE READ` 隔离级别下

**使用 `REPEATABLE READ` 隔离级别的事务来说，只会在第一次执行查询语句时生成一个 ReadView，之后的查询就不会重复生成了。**

![image-20220310161613246](image-20220310161613246.png)

![image-20220310161627167](image-20220310161627167.png)

![image-20220310161637175](image-20220310161637175.png)

![image-20220310161712522](image-20220310161712522.png)

![image-20220310161728270](image-20220310161728270.png)

![image-20220310161754102](image-20220310161754102.png)

#### 11.5.3 如何解决幻读

**注意：解决幻读的大前提是在 `REPEATABLE READ` 隔离级别下进行演示的。**

![image-20220310162020665](image-20220310162020665.png)

![image-20220310162208731](image-20220310162208731.png)

![image-20220310162222328](image-20220310162222328.png)

### 11.6 总结

这里介绍了 MVCC 在 `READ COMMITTD`、`REPEATABLE READ` 这两种隔离级别的事务在执行快照读操作时访问记录的版本链的过程。这样使不同事务的**读-写、写-读操作并发执行**，从而提升系统性能。

核心点在于 ReadView 的原理，`READ COMMITTD`、`REPEATABLE READ` 这两个隔离级别的一个很大不同就是生成 ReadView 的时机不同：

- `READ COMMITTD` 在每一次进行普通 `SELECT` 操作前都会生成一个 ReadView
- `REPEATABLE READ` 只在第一次进行普通 `SELECT` 操作前生成一个 ReadView，之后的查询操作都重复使用这个 ReadView 就好了

说明：我们之前说执行 `DELETE` 语句或者更新主键的 `UPDATE` 语句并不会立即把对应的记录完全从页面中删除，而是执行一个所谓的 delete mark 操作，相当于只是对记录打上了一个删除标志位，这主要就是为了 MVCC 服务的。

通过 MVCC 我们可以解决：

- 读写之间阻塞的问题。
- 降低了死锁的概率。（因为 MVCC 采用了乐观锁的方式，读取数据并不需要加锁，对于写操作也只锁定必要的行）
- 解决快照读的问题。
